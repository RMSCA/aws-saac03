[{"text": "40m\n\nQ596 A company provides an API interface to customers so the customers can retrieve their financial information.The\ncompany expects a larger number of requests during peak usage times of the year.The company requires the API to\nrespond consistently with low latency to ensure customer satisfaction.The company needs to provide a compute host for\nthe API.Which solution will meet these requirements with the LEAST operational overhead?\n\nA. Use an Application Load Balancer and Amazon Elastic Container Service (Amazon ECS).\n\nB. Use Amazon API Gateway and AWS Lambda functions with provisioned concurrency.\n\nC. Use an Application Load Balancer and an Amazon Elastic Kubernetes Service (Amazon EKS) cluster.\n\nD. Use Amazon API Gateway and AWS Lambda functions with reserved concurrency.\n", "image": "Screenshot 2024-12-27 at 12.00.33.png"}, {"text": "S168\n\nQ276 A company has a web application with sporadic usage patterns.There is heavy usage at the beginning of each\nmonth, moderate usage at the start of each week, and unpredictable usage during the week.The application consists of a\nweb server and a MySQL database server running inside the data center.The company would like to move the application\nto the AWS Cloud, and needs to select a cost-effective database platform that will not require database\nmodifications.Which solution will meet these requirements?\n\nA. Amazon DynamoDB\n\nB. Amazon RDS for MySQL\n\nCc. MySQL-compatible Amazon Aurora Serverless\n\nD. MySQL deployed on Amazon EC2 in an Auto Scaling group\n", "image": "Screenshot 2024-12-27 at 11.40.33.png"}, {"text": "5B11288\n\nQ19 A company stores user data in AWS. The data is used continuously with peak usage during business hours. Access\npatterns vary, with some data not being used for months at a time. A solutions architect must choose a cost-effective\nsolution that maintains the highest level of durability while maintaining high availability.Which storage solution meets these\nrequirements?\n\nA. Amazon S3 Standard\n\nB. Amazon S3 Intelligent-Tiering\n\nC. Amazon S3 Glacier Deep Archive\n\nD. Amazon S3 One Zone-Infrequent Access (S3 One Zone-IA)\n", "image": "Screenshot 2024-12-27 at 12.08.57.png"}, {"text": "148\n\nQ1013 A company's production environment consists of Amazon EC2 On-Demand Instances that run constantly between\nMonday and Saturday.The instances must run for only 12 hours on Sunday and cannot tolerate interruptions. The company\nwants to cost-optimize the production environment.Which solution will meet these requirements MOST cost-effectively?\n\nA. Purchase Scheduled Reserved Instances for the EC2 instances that run for only 12 hours on Sunday.Purchase\n\nStandard Reserved Instances for the EC2 instances that run constantly between Monday and Saturday.\n\nB. Purchase Convertible Reserved Instances for the EC2 instances that run for only 12 hours on Sunday.Purchase\nStandard Reserved Instances for the EC2 instances that run constantly between Monday and Saturday.\n\nc. Use Spot Instances for the EC2 instances that run for only 12 hours on Sunday.Purchase Standard Reserved\nInstances for the EC2 instances that run constantly between Monday and Saturday.\n\nD. Use Spot Instances for the EC2 instances that run for only 12 hours on Sunday.Purchase Convertible Reserved\nInstances for the EC2 instances that run constantly between Monday and Saturday.\n", "image": "Screenshot 2024-12-27 at 12.12.32.png"}, {"text": "S113\n\nQ20 A company wants to replicate existing and ongoing data changes from an on-premises Oracle database to Amazon\nRDS for Oracle. The amount of data to replicate varies throughout each day. The company wants to use AWS Database\nMigration Service (AWS DMS) for data replication. The solution must allocate only the capacity that the replication instance\nrequires.Which solution will meet these requirements?\n\nA. Configure the AWS DMS replication instance with a Multi-AZ deployment to provision instances across multiple\nAvailability Zones.\n\nB. Create an AWS DMS Serverless replication task to analyze and replicate the data while provisioning the required\n\ncapacity.\n\nC. Use Amazon EC2 Auto Scaling to scale the size of the AWS DMS replication instance up or down based on the\namount of data toreplicate.\n\nD. Provision AWS DMS replication capacity by using Amazon Elastic Container Service (Amazon ECS) with an AWS\nFargate launch type to analyze and replicate the data while provisioning the required capacity.\n", "image": "Screenshot 2024-12-27 at 12.09.02.png"}, {"text": "#1230\n\nQ30 A company needs to give a globally distributed development team secure access to the company's AWS resources in\na way that complies with security policies. The company currently uses an on-premises Active Directory for internal\nauthentication.The company uses AWS Organizations to manage multiple AWS accounts that support multiple projects.The\ncompany needs a solution to integrate with the existing infrastructure to provide centralized identity management and\naccess control.Which solution will meet these requirements with the LEAST operational overhead?\n\nA. Set up AWS Directory Service to create an AWS managed Microsoft Active Directory on AWS.Establish a trust\nrelationship with the onpremises Active Directory.Use IAM rotes that are assigned to Active Directory groups to\naccess AWS resources within the company's AWS accounts.\n\nB. Create an IAM user for each developer.Manually manage permissions for each IAM user based on each user's\ninvolvement with each project.Enforce multi-factor authentication (MFA) as an additional layer of security.\n\nc.Use AD Connector in AWS Directory Service to connect to the on-premises Active Directory.Integrate AD Connector\n\nwith AWS IAM Identity Center.Configure permissions sets to give each AD group access to specific AWS accounts\nand resources.\n\nD. Use Amazon Cognito to deploy an identity federation solution.Integrate the identity federation solution with the on-\npremises Active Directory.Use Amazon Cognito to provide access tokens for developers to access AWS accounts\nand resources.\n", "image": "Screenshot 2024-12-27 at 12.10.23.png"}, {"text": "S125\n\nQ32 A company has a multi-tier web application.The application's internal service components are deployed on Amazon\nEC2 instances.The internal service components need to access third-party software as a service (SaaS) APIs that are\nhosted on AWS.The company needs to provide secure and private connectivity from the application's internal services to\nthe third-party SaaS application. The company needs to ensure that there is minimal public internet exposure.Which\n\nsolution will meet these requirements?\n\nA. Implement an AWS Site-to-Site VPN to establish a secure connection with the third-party SaaS provider.\n\nB. Deploy AWS Transit Gateway to manage and route trac between the application's VPC and the third-party SaaS\nprovider.\n\nc. Congure AWS PrivateLink to allow only outbound trac from the VPC without enabling the third-party SaaS provider to\nestablish.\n\nD. Use AWS PrivateLink to create a private connection between the application's VPC and the third-party SaaS\n\nprovider.\n\n", "image": "Screenshot 2024-12-27 at 12.10.37.png"}, {"text": "meow\nQ789 The DNS provider that hosts a company's domain name records is experiencing outages that cause service\ndisruption for a website running on AWS. The company needs to migrate to a more resilient managed DNS service and\n\nwants the service to run on AWS. What should a solutions architect do to rapidly migrate the DNS hosting service?\n\nA. Create an Amazon Route 53 public hosted zone for the domain name. Import the zone file containing the domain\n\nrecords hosted by the previous provider.\n\nB. Create an Amazon Route 53 private hosted zone for the domain name. Import the zone file containing the domain\nrecords hosted by the previous provider.\n\nC. Create a Simple AD directory in AWS. Enable zone transfer between the DNS provider and AWS Directory Service for\nMicrosoft Active Directory for the domain records.\n\nD. Create an Amazon Route 53 Resolver inbound endpoint in the VPC. Specify the IP addresses that the provider's DNS\nwill forward DNS queries to. Configure the provider's DNS to forward DNS queries for the domain to the IP addresses\nthat are specified in the inbound endpoint.\n", "image": "Screenshot 2024-12-27 at 12.04.40.png"}, {"text": "S188\n\nQ25 A company is designing the architecture for a new mobile app that uses the AWS Cloud.The company uses\norganizational units (OUs) in AWS Organizations to manage its accounts.The company wants to tag Amazon EC2 instances\nwith data sensitivity by using values of sensitive and nonsensitive.IAM identities must not be able to delete a tag or create\ninstances without a tag.Which combination of steps will meet these requirements? (Choose two.)\n\nA. In Organizations, create a new tag policy that specifies the data sensitivity tag key and the required values.Enforce\n\nthe tag values for the EC2 instances.Attach the tag policy to the appropriate OU.\n\nB. In Organizations, create a new service control policy (SCP) that specifies the data sensitivity tag key and the required\ntag values.Enforce the tag values for the EC2 instances.Attach the SCP to the appropriate OU.\n\nC. Create a tag policy to deny running instances when a tag key is not specified.Create another tag policy that prevents\nidentities from deleting tags.Attach the tag policies to the appropriate OU.\n\nD. Create a service control policy (SCP) to deny creating instances when a tag key is not specified.Create another SCP\n\nthat prevents identities from deleting tags.Attach the SCPs to the appropriate OU.\n\nE. Create an AWS Config rule to check if EC2 instances use the data sensitivity tag and the specified values.Configure\nan AWS Lambda function to delete the resource if a noncompliant resource is found.\n", "image": "Screenshot 2024-12-27 at 12.09.28.png"}, {"text": "B38a\nQ571 A solutions architect is designing an asynchronous application to process credit card data validation requests for a\n\nbank.The application must be secure and be able to process each request at least once.Which solution will meet these\nrequirements MOST cost-effectively?\n\nA. Use AWS Lambda event source mapping.Set Amazon Simple Queue Service (Amazon SQS) standard queues as the\n\nevent source.Use AWS Key Management Service (SSE-KMS) for encryption.Add the kms:Decrypt permission for the\nLambda execution role.\n\nB. Use AWS Lambda event source mapping.Use Amazon Simple Queue Service (Amazon SQS) FIFO queues as the\nevent source.Use SQS managed encryption keys (SSE-SQS) for encryption.Add the encryption key invocation\npermission for the Lambda function.\n\nc. Use the AWS Lambda event source mapping.Set Amazon Simple Queue Service (Amazon SQS) FIFO queues as the\nevent source.Use AWS KMS keys (SSE-KMS).Add the kms:Decrypt permission for the Lambda execution role.\n\nD. Use the AWS Lambda event source mapping.Set Amazon Simple Queue Service (Amazon SQS) standard queues as\nthe event source.Use AWS KMS keys (SSE-KMS) for encryption.Add the encryption key invocation permission for the\nLambda function.\n", "image": "Screenshot 2024-12-27 at 12.00.18.png"}, {"text": "920\n\nQ904 A company is migrating a legacy application from an on-premises data center to AWS. The application relies on\nhundreds of cron jobs that run between 1 and 20 minutes on different recurring schedules throughout the day. The\ncompany wants a solution to schedule and run the cron jobs on AWS with minimal refactoring. The solution must support\nrunning the cron jobs in response to an event in the future. Which solution will meet these requirements?\n\nA. Create a container image for the cron jobs. Use Amazon EventBridge Scheduler to create a recurring schedule. Run\nthe cron job tasks as AWS Lambda functions.\n\nB. Create a container image for the cron jobs. Use AWS Batch on Amazon Elastic Container Service (Amazon ECS) with\na scheduling policy to run the cron jobs.\n\nC. Create a container image for the cron jobs. Use Amazon EventBridge Scheduler to create a recurring schedule. Run\n\nthe cron job tasks on AWS Fargate.\n\nD. Create a container image for the cron jobs. Create a workflow in AWS Step Functions that uses a Wait state to run the\ncron jobs at a specified time. Use the RunTask action to run the cron job tasks on AWS Fargate.\n", "image": "Screenshot 2024-12-27 at 12.07.11.png"}, {"text": "28a\n\nQ476 A company is running a multi-tier recommence web application in the AWS Cloud.The application runs on Amazon\nEC2 instances with an Amazon RDS for MySQL Multi-AZ OB instance.Amazon ROS is configured with the latest generation\nDB instance with 2.000 GB of storage.In a General Purpose SSD (gp3) Amazon Elastic Block Store (Amazon EBSI\nvolume.The database performance affects the application during periods high demand.A database administrator analyzes\nthe logs in Amazon CloudWatch Logs and discovers that the application performance always degrades when the number\nof read and write IOPS is higher than 20.000.What should a solutions architect do to improve the application performance?\n\nA. Replace the volume with a magnetic volume.\nB. Increase the number of IOPS on the gp3 volume.\nC. Replace the volume with a Provisioned IOPS SSD.(lo2) volume.\n\nD. Replace the 2.000 GB gp3 volume with two 1.000 GB gp3 volumes\n", "image": "Screenshot 2024-12-27 at 11.42.21.png"}, {"text": "10988\n\nQ16 A company is using AWS DataSync to migrate millions of files from an on-premises system to AWS. The files are 10 KB\nin size on average.The company wants to use Amazon S3 for file storage. For the first year after the migration, the files will\nbe accessed once or twice and must be immediately available. After 1 year, the files must be archived for at least 7\nyears.Which solution will meet these requirements MOST cost-effectively?\n\nA. Use an archive tool to group the files into large objects. Use DataSync to migrate the objects. Store the objects in S3\nGlacier Instant Retrieval for the first year. Use a lifecycle configuration to transition the files to S3 Glacier Deep\nArchive after 1 year with a retention period of 7 years.\n\nB. Use an archive tool to group the files into large objects. Use DataSync to copy the objects to S3 Standard-Infrequent\nAccess (S3 Standard-lA). Use a lifecycle configuration to transition the files to S3 Glacier Instant Retrieval after 1 year\nwith a retention period of 7 years.\n\nC. Configure the destination storage class for the files as S3 Glacier Instant Retrieval. Use a lifecycle policy to transition\nthe files to S3 Glacier Flexible Retrieval after 1 year with a retention period of 7 years.\n\nD. Configure a DataSync task to transfer the files to S3 Standard-Infrequent Access (S3 Standard-lA). Use a lifecycle\n\nconfiguration to transition the files to S3 Deep Archive after 1 year with a retention period of 7 years.\n\n", "image": "Screenshot 2024-12-27 at 12.08.41.png"}, {"text": "158i\n\nQ242 A company is designing a cloud communications platform that is driven by APls.The application is hosted on Amazon\nEC2 instances behind a Network Load Balancer (NLB).The company uses Amazon API! Gateway to provide external users\nwith access to the application through APIs.The company wants to protect the platform against web exploits like SQL\ninjection and also wants to detect and mitigate large, sophisticated DDoS attacks.Which combination of solutions provides\nthe MOST protection? (Select TWO.)\n\nA. Use AWS WAF to protect the NLB.\nD. Use Amazon GuardDuty with AWS Shield Standard.\n\nE. Use AWS Shield Standard with Amazon API Gateway.\n", "image": "Screenshot 2024-12-27 at 11.40.25.png"}, {"text": "30m\n\nQ575 A company is conducting an internal audit.The company wants to ensure that the data in an Amazon S3 bucket that\nis associated with the company's AWS Lake Formation data lake does not contain sensitive customer or employee\ndata.The company wants to discover personally identifiable information (PII) or financial information, including passport\nnumbers and credit card numbers.Which solution will meet these requirements?\n\nA. Configure AWS Audit Manager on the account.Select the Payment Card Industry Data Security Standards (PCI DSS)\nfor auditing.\n\nB. Configure Amazon S3 Inventory on the S3 bucket Configure Amazon Athena to query the inventory.\n\nc. Configure Amazon Macie to run a data discovery job that uses managed identifiers for the required data types.\n\nD. Use Amazon S3 Select to run a report across the S3 bucket.\n", "image": "Screenshot 2024-12-27 at 12.00.25.png"}, {"text": "so1gl\n\nQ901 A marketing team wants to build a campaign for an upcoming multi-sport event. The team has news reports from the\npast five years in PDF format. The team needs a solution to extract insights about the content and the sentiment of the\nnews reports. The solution must use Amazon Textract to process the news reports. Which solution will meet these\nrequirements with the LEAST operational overhead?\n\nA. Provide the extracted insights to Amazon Athena for analysis. Store the extracted insights and analysis in an Amazon\nS3 bucket.\n\nB. Store the extracted insights in an Amazon DynamoDB table. Use Amazon SageMaker to build a sentiment model.\n\nC. Provide the extracted insights to Amazon Comprehend for analysis. Save the analysis to an Amazon S3 bucket.\n\nD. Store the extracted insights in an Amazon S3 bucket. Use Amazon QuickSight to visualize and analyze the data.\n", "image": "Screenshot 2024-12-27 at 12.07.04.png"}, {"text": "Q503 A solutions architect wants to use the following JSON text as an identity-based policy to grant specific permissions:\nWhich IAM principals can the solutions architect attach this policy to? (Choose two.)\n{ \u201cStatement\u201d:\n{\n\"Action\":\n\u201cssm:ListDocuments\u201d,\n\n\"\u201cssm:GetDocument\n\n\"Effect\": \u201cAllow\u201d,\n\n\u201cResource\u201d: P\n\n\"Sid*: \u201c7\n\nly\n\n\u201cVersion\u201d: %2012-10-17\"\n\nA. Role\n\nB. Group\n\nC. Organization\n\nD. Amazon Elastic Container Service (Amazon ECS) resource\n\nE. Amazon EC2 resource\n", "image": "Screenshot 2024-12-27 at 11.48.55.png"}, {"text": "5B 12458\n\nQ31 A solutions architect is designing the storage architecture for a new web application used for storing and viewing\nengineering drawings.All application components will be deployed on the AWS infrastructure.The application design must\nsupport caching to minimize the amount of time that users wait for the engineering drawings to load.The application must\nbe able to store petabytes of data.Which combination of storage and caching should the solutions architect use?\n\nA. Amazon S3 with Amazon CloudFront\n\nB. Amazon S3 Glacier with Amazon ElastiCache\nc. Amazon Elastic Block Store (Amazon EBS) volumes with Amazon CloudFront\n\nD. AWS Storage Gateway with Amazon ElastiCache\n", "image": "Screenshot 2024-12-27 at 12.10.30.png"}, {"text": "sea\n\nQ144 A company has a three-tier web application that is deployed on AWS.The web servers are deployed in a public\nsubnet in a VPC.The application servers and database servers are deployed in private subnets in the same VPC.The\ncompany has deployed a third-party virtual firewall appliance from AWS Marketplace in an inspection VPC.The appliance is\nconfigured with an IP interface that can accept IP packets.A solutions architect needs to Integrate the web application with\nthe appliance to inspect all traffic to the application before the traffic teaches the web server.Which solution will moot\n\nthese requirements with the LEAST operational overhead?\n\nA. Create a Network Load Balancer the public subnet of the application's VPC to route the traffic to the appliance for\npacket inspection.\n\nB. Create an Application Load Balancer in the public subnet of the application's VPC to route the traffic to the appliance\nfor packet inspection.\n\nC. Deploy a transit gateway in the inspection VPC.Configure route tables to route the incoming pockets through the\ntransit gateway.\n\nD. Deploy a Gateway Load Balancer in the inspection VPC.Create a Gateway Load Balancer endpoint to receive the\n\nincoming packets and forward the packets to the appliance.\n\n", "image": "Screenshot 2024-12-27 at 11.36.28.png"}, {"text": "52980\n\nQ480 A company needs to transfer 600 TB of data from its on-premises network-attached storage (NAS) system to the\nAWS Cloud.The data transfer must be complete within 2 weeks.The data is sensitive and must be encrypted in transit.The\ncompany's internet connection can support an upload speed of 100 Mbps.Which solution meets these requirements MOST\n\ncost-effectively?\n\nA. Use Amazon S3 multi-part upload functionality to transfer the fees over HTTPS.\n\nB. Create a VPN connection between the on-premises NAS system and the nearest AWS Region.Transfer the data over\nthe VPN connection.\n\nc. Use the AWS Snow Family console to order several AWS Snowball Edge Storage Optimized devices.Use the devices\n\nto transfer the data to Amazon S3.\n\nD. Set up a 10 Gbps AWS Direct Connect connection between the company location and the nearest AWS\nRegion.Transfer the data over a VPN connection into the Region to store the data in Amazon S3.\n", "image": "Screenshot 2024-12-27 at 11.42.30.png"}, {"text": "370\n\nQ543 An loT company is releasing a mattress that has sensors to collect data about a user's sleep.The sensors will send\ndata to an Amazon S3 bucket.The sensors collect approximately 2 MB of data every night for each mattress.The company\nmust process and summarize the data for each mattress.The results need to be available as soon as possible.Data\nprocessing will require 1 GB of memory and will finish within 30 seconds.Which solution will meet these requirements\nMOST cost-effectively?\n\nA. Use AWS Glue with a Scala job\n\nB. Use Amazon EMR with an Apache Spark script\n\nCc. Use AWS Lambda with a Python script\n\nD. Use AWS Glue with a PySpark job\n", "image": "Screenshot 2024-12-27 at 12.00.09.png"}, {"text": "SF 8oRH\n\nQ899 An online gaming company hosts its platform on Amazon EC2 instances behind Network Load Balancers (NLBs)\nacross multiple AWS Regions. The NLBs can route requests to targets over the internet. The company wants to improve\nthe customer playing experience by reducing end-to-end load time for its global customer base. Which solution will meet\n\nthese requirements?\n\nA. Create Application Load Balancers (ALBs) in each Region to replace the existing NLBs. Register the existing EC2\ninstances as targets for the ALBs in each Region.\n\nB. Configure Amazon Route 53 to route equally weighted traffic to the NLBs in each Region.\n\nCc. Create additional NLBs and EC2 instances in other Regions where the company has large customer bases.\n\nD. Create a standard accelerator in AWS Global Accelerator. Configure the existing NLBs as target endpoints.\n", "image": "Screenshot 2024-12-27 at 12.06.54.png"}, {"text": "551118\n\nQ18 A solutions architect is creating an application that will handle batch processing of large amounts of data.The input\ndata will be held in Amazon S3 and the output data will be stored in a different S3 bucket.For processing, the application\nwill transfer the data over the network between multiple Amazon EC2 instances.What should the solutions architect do to\nreduce the overall data transfer costs?\n\nA. Place all the EC2 instances in an Auto Scaling group.\n\nB. Place all the EC2 instances in the same AWS Region.\n\nC. Place all the EC2 instances in the same Availability Zone.\n\nD. Place all the EC2 instances in private subnets in multiple Availability Zones.\n", "image": "Screenshot 2024-12-27 at 12.08.51.png"}, {"text": "551208\n\nQ27 A company runs a Node js function on a server in its on-premises data center.The data center stores data ina\nPostgreSQL database.The company stores the credentials in a connection string in an environment variable on the\nserver.The company wants to migrate its application to AWS and to replace the Node.js application server with AWS\nLambda.The company also wants to migrate to Amazon RDS for PostgreSQL and to ensure that the database credentials\nare securely managed.Which solution will meet these requirements with the LEAST operational overhead?\n\nA. Store the database credentials as a parameter in AWS Systems Manager Parameter Store Congure Parameter Store\nto automatically rotate the secrets every 30 days.Update the Lambda function to retrieve the credentials from the\nparameter.\n\nB. Store the database credentials as a secret in AWS Secrets Manager.Congure Secrets Manager to automatically\n\nrotate the credentials every 30 days.Update the Lambda function to retrieve the credentials from the secret.\n\nCc. Store the database credentials as an encrypted Lambda environment variable.Write a custom Lambda function to\nrotate the credentials.Schedule the Lambda function to run every 30 days.\n\nD. Store the database credentials as a key in AWS Key Management Service (AWS KMS).Congure automatic rotation for\nthe key.Update the Lambda function to retneve the credentials from the KMS key.\n", "image": "Screenshot 2024-12-27 at 12.09.38.png"}, {"text": "4g\n\nQ56 A company runs its ecommerce application on AWS.Every new order is published as a message in a RabbitWQ queue\nthat runs on an Amazon EC2 instance in a single Availability Zone-These messages are processed by a different application\nthat runs on a separate EC2 instance.This application stores the details in a PostgreSQL database on another EC2\ninstance.All the EC2 instances are in the same Availability Zone.The company needs to redesign its architecture to provide\nthe highest availability with the least operational overhead.What should a solutions architect do to meet these\n\nrequirements?\nA. Migrate the queue to a redundant pair (active/standby) of RabbitMQ instances on Amazon MQ.Create a Multi-AZ\n\nAuto Scaling group (or EC2 instances that host the application.Create another Multi-AZ Auto Scaling group for EC2\ninstances that host the PostgreSQL database.\n\nB. Migrate the queue to a redundant pair (active/standby) of RabbitMQ instances on Amazon MQ.Create a Multi-AZ\n\nAuto Scaling group for EC2 instances that host the application.Migrate the database to run on a Multi-AZ\ndeployment of Amazon RDS for PostgreSQL.\n\nC. Create a Multi-AZ Auto Scaling group for EC2 instances that host the RabbitWMQ queue.Create another Multi-AZ Auto\nScaling group for EC2 instances that host the application.Migrate the database to run on a Multi-AZ deployment of\nAmazon RDS fqjPostgreSQL.\n\nD. Create a Multi-AZ Auto Scaling group for EC2 instances that host the RabbitMQ queue.Create another Multi-AZ Auto\nScaling group for EC2 instances that host the application.Create a third Multi-AZ Auto Scaling group for EC2\ninstances that host the PostgreSQL database.\n", "image": "Screenshot 2024-12-27 at 11.35.50.png"}, {"text": "5567\n\nQ790 A company is building an application on AWS that connects to an Amazon RDS database. The company wants to\nmanage the application configuration and to securely store and retrieve credentials for the database and other services.\nWhich solution will meet these requirements with the LEAST administrative overhead?\n\nA. Use AWS AppConfig to store and manage the application configuration. Use AWS Secrets Manager to store and\n\nretrieve the credentials.\n\nB. Use AWS Lambda to store and manage the application configuration. Use AWS Systems Manager Parameter Store to\nstore and retrieve the credentials.\n\nc. Use an encrypted application configuration file. Store the file in Amazon S3 for the application configuration. Create\nanother S3 file to store and retrieve the credentials.\n\nD. Use AWS AppConfig to store and manage the application configuration. Use Amazon RDS to store and retrieve the\ncredentials.\n", "image": "Screenshot 2024-12-27 at 12.04.46.png"}, {"text": "ese\n\nQ798 A company sets up an organization in AWS Organizations that contains 10 AWS accounts. A solutions architect must\ndesign a solution to provide access to the accounts for several thousand employees. The company has an existing identity\nprovider (IdP). The company wants to use the existing IdP for authentication to AWS. Which solution will meet these\nrequirements?\n\nA. Create IAM users for the employees in the required AWS accounts. Connect IAM users to the existing IdP. Configure\nfederated authentication for the IAM users.\n\nB. Set up AWS account root users with user email addresses and passwords that are synchronized from the existing\nldP.\n\nc. Configure AWS IAM Identity Center (AWS Single Sign-On). Connect IAM Identity Center to the existing IdP. Provision\n\nusers and groups from the existing IdP.\n\nD. Use AWS Resource Access Manager (AWS RAM) to share access to the AWS accounts with the users in the existing\nIdP.\n", "image": "Screenshot 2024-12-27 at 12.04.52.png"}, {"text": "B71\n\nQ813 A development team is collaborating with another company to create an integrated product. The other company\nneeds to access an Amazon Simple Queue Service (Amazon SQS) queue that is contained in the development team's\naccount. The other company wants to poll the queue without giving up its own account permissions to do so. How should a\nsolutions architect provide access to the SQS queue?\n\nA. Create an instance profile that provides the other company access to the SQS queue.\nB. Create an IAM policy that provides the other company access to the SQS queue.\n\nc. Create an SQS access policy that provides the other company.access to the SQS queue.\n\nD. Create an Amazon Simple Notification Service (Amazon SNS) access policy that provides the other company access\nto the SQS queue.\n", "image": "Screenshot 2024-12-27 at 12.05.12.png"}, {"text": "708\n\nQ803 A company's data platform uses an Amazon Aurora MySQL database. The database has multiple read replicas and\nmultiple DB instances across different Availability Zones. Users have recently reported errors from the database that\nindicate that there are too many connections. The company wants to reduce the failover time by 20% when a read replica\nis promoted to primary writer. Which solution will meet this requirement?\n\nA. Switch from Aurora to Amazon RDS with Multi-AZ cluster deployment.\n\nB. Use Amazon RDS Proxy in front of the Aurora database.\n\nCc. Switch to Amazon DynamoDB with DynamoDB Accelerator (DAX) for read connections.\n\nD. Switch to Amazon Redshift with relocation capability.\n", "image": "Screenshot 2024-12-27 at 12.05.06.png"}, {"text": "5587\n\nQ893 A company uses an AWS Batch job to run its end-of-day sales process. The company needs a serverless solution\nthat will invoke a third-party reporting application when the AWS Batch job is successful. The reporting application has an\nHTTP API interface that uses username and password authentication. Which solution will meet these requirements?\n\nA. Configure an Amazon EventBridge rule to match incoming AWS Batch job SUCCEEDED events. Configure the third-\nparty API as an EventBridge API destination with a username and password. Set the API destination as the\nEventBridge rule target.\n\nB. Configure Amazon EventBridge Scheduler to match incoming AWS Batch job SUCCEEDED events. Configure an\n\nAWS Lambda function to invoke the third-party API by using a username and password. Set the Lambda function as\nthe EventBridge rule target.\n\nCc. Configure an AWS Batch job to publish job SUCCEEDED events to an Amazon API Gateway REST API. Configure an\nHTTP proxy integration on the API Gateway REST API to invoke the third-party API by using a username and\npassword.\n\nD. Configure an AWS Batch job to publish job SUCCEEDED events to an Amazon API Gateway REST API. Configure a\nproxy integration on the API Gateway REST API to an AWS Lambda function. Configure the Lambda function to\ninvoke the third-party API by using a username and password.\n", "image": "Screenshot 2024-12-27 at 12.06.43.png"}, {"text": "S146\n\nQ1021 A solutions architect is designing a three-tier web application.The architecture consists of an internet-facing\nApplication Load Balancer (ALB) and a web tier that is hosted on Amazon EC2 instances in private subnets.The application\ntier with the business logic runs on EC2 instances in private subnets.The database tier consists of Microsoft SQL Server\nthat runs on EC2 instances in private subnets.Security is a high priority for the company.Which combination of security\ngroup Configurations should the solutions architect use? (Choose three.)\n\nA. Configure the security group for the web tier to allow inbound HTTPS traffic from the security group for the ALB.\n\nB. Configure the security group for the web tier to allow outbound HTTPS traffic to 0.0.0.0/0.\n\nc. Configure the security group for the database tier to allow inbound Microsoft SQL Server traffic from the security\n\ngroup for the application tier.\n\nD. Configure the security group for the database tier to allow outbound HTTPS traffic and Microsoft SQL Server traffic\nto the security group for the web tier.\n\nE. Configure the security group for the application tier to allow inbound HTTPS traffic from the security group for the\n\nweb tier.\n\nF. Configure the security group for the application tier to allow outbound HTTPS traffic and Microsoft SQL Server traffic\nto the security group for the web tier.\n", "image": "Screenshot 2024-12-27 at 12.12.22.png"}, {"text": "S108\n\nQ17 A company is migrating its on-premises Oracle database to an Amazon RDS for Oracle database. The company needs\nto retain data for 90 days to meet regulatory requirements. The company must also be able to restore the database to a\nspecific point in time for up to 14 days.Which solution will meet these requirements with the LEAST operational overhead?\n\nA. Create Amazon RDS automated backups. Set the retention period to 90 days.\nB. Create an Amazon RDS manual snapshot every day. Delete manual snapshots that are older than 90 days.\n\nc. Use the Amazon Aurora Clone feature for Oracle to create a point-in-time restore. Delete clones that are older than\n90 days.\n\nD. Create a backup plan that has a retention period of 90 days by using AWS Backup for Amazon RDS.\n", "image": "Screenshot 2024-12-27 at 12.08.46.png"}, {"text": "#14988\n\nQ1006 A company uses AWS Systems Manager for routine management and patching of Amazon EC2 instances.The EC2\ninstances are in an IP address type target group behind an Application Load Balancer (ALB).New security protocols require\nthe company to remove EC2 instances from service during a patch.When the company attempts to follow the security\nprotocol during the next patch, the company receives errors during the patching window.Which combination of solutions\nwill resolve the errors? (Choose two.)\n\nA. Change the target type of the target group from IP address type to instance type.\n\nB. Continue to use the existing Systems Manager document without changes because it is already optimized to handle\ninstances that are in an IP address type target group behind an ALB.\n\nCc. Implement the AWSEC2-PatchLoadBalanacerlInstance Systems Manager Automation document to manage the\npatching process.\n\nD. Use Systems Manager Maintenance Windows to automatically remove the instances from service to patch the\ninstances.\n\nE. Configure Systems Manager State Manager to remove the instances from service and manage the patching\nschedule.Use ALB health checks to re-route traffic.\n", "image": "Screenshot 2024-12-27 at 12.12.37.png"}, {"text": "383180\n\nQ507 A solutions architect is implementing a complex Java application with a MySQL database.The Java application must\nbe deployed on Apache Tomcat and must be highly available.What should the solutions architect do to meet these\nrequirements?\n\nA. Deploy the application in AWS Lambda.Configure an Amazon API Gateway API to connect with the Lambda\nfunctions.\n\nB. Deploy the application by using AWS Elastic Beanstalk.Configure a load-balanced environment and a rolling\n\ndeployment policy.\n\nC. Migrate the database to Amazon ElastiCache.Configure the ElastiCache security group to allow access from the\napplication.\n\nD. Launch an Amazon EC2 instance.Install a MySQL server on the EC2 instance.Configure the application on the\nserver.Create an AMI.Use the AMI to create a launch template with an Auto Scaling group.\n", "image": "Screenshot 2024-12-27 at 11.49.07.png"}, {"text": "93m\n\nQ929 A company has 15 employees. The company stores employee start dates in an Amazon DynamoDB table. The\ncompany wants to send an email message to each employee on the day of the employee's work anniversary. Which\nsolution will meet these requirements with the MOST operational efficiency?\n\nA. Create a script that scans the DynamoDB table and uses Amazon Simple Notification Service (Amazon SNS) to send\nemail messages to employees when necessary. Use a cron job to run this script every day on an Amazon EC2\ninstance.\n\nB. Create a script that scans the DynamoDB table and uses Amazon Simple Queue Service (Amazon SQS) to send\nemail messages to employees when necessary. Use a cron job to run this script every day on an Amazon EC2\ninstance.\n\nCc. Create an AWS Lambda function that scans the DynamoDB table and uses Amazon Simple Notification Service\n\n(Amazon SNS) to send email messages to employees when necessary. Schedule this Lambda function to run every\nday.\n\nD. Create an AWS Lambda function that scans the DynamoDB table and uses Amazon Simple Queue Service (Amazon\nSQS) to send email messages to employees when necessary. Schedule this Lambda function to run every day.\n", "image": "Screenshot 2024-12-27 at 12.07.16.png"}, {"text": "5B115\n\nQ22 A company is planning to migrate a legacy application to AWS.The application currently uses NFS to communicate to\nan on-premises storage solution to store application data.The application cannot be modified to use any other\ncommunication protocols other than NFS for this purpose.Which storage solution should a solutions architect recommend\n\nfor use after the migrations?\n\nA. AWS DataSync\n\nB. Amazon Elastic Block Store (Amazon EBS)\n\nc. Amazon Elastic File System (Amazon EFS)\n\nD. Amazon EMR File System (Amazon EMRFS)\n", "image": "Screenshot 2024-12-27 at 12.09.13.png"}, {"text": "B14\n\nQ21 A company uses GPS trackers to document the migration patterns of thousands of sea turtles.The trackers check\nevery 5 minutes to see if a turtle has moved more than 100 yards(91.4 meters) .If a turtle has moved,its tracker sends the\nnew coordinates to a web application running on three Amazon EC2 instances that are in multiple Availability Zones in one\nAWS RegionRecently,the web application was overwhelmed while processing an unexpected volume of tracker data.Data\nwas lost with no way to replay the events.A solutions architect must prevent this problem from happening again and needs\na solution with the least operational overhead.What should the solutions architect do to meet these requirements?\n\nA. Create an Amazon S3 bucket to store the data Configure the application to scan for new data in the bucket for\nprocessing.\n\nB. Create an Amazon API Gateway endpoint to handle transmitted location coordinates.Use an AWS Lambda function to\nprocess each item concurrently.\n\nc. Create an Amazon Simple Queue Service (Amazon SQS) queue to store the incoming data Configure the application\n\nto poll for new messages for processing.\n\nD. Create an Amazon DynamoDB table to store transmitted location coordinates.Configure the application to query the\ntable for new data for processing.Use TTL to remove data that has been processed.\n", "image": "Screenshot 2024-12-27 at 12.09.07.png"}, {"text": "1988\n\nQ313 A company hosts its application on AWS. The company uses Amazon Cognito to manage users. When users log in to\nthe application, the application fetches required data from Amazon DynamoDB by using a REST API that is hosted in\nAmazon API Gateway. The company wants an AWS managed solution that will control access to the REST API to reduce\ndevelopment efforts.Which solution will meet these requirements with the LEAST operational overhead?\n\nA. Configure an AWS Lambda function to be an authorizer in API Gateway to validate which user made the request.\n\nB. For each user, create and assign an API key that must be sent with each request. Validate the key by using an AWS\nLambda function.\n\nc. Send the user's email address in the header with every request. Invoke an AWS Lambda function to validate that the\nuser with that email address has proper access.\n\nD. Configure an Amazon Cognito user pool authorizer in API Gateway to allow Amazon Cognito to validate each request.\n", "image": "Screenshot 2024-12-27 at 11.41.04.png"}, {"text": "Gomi\n\nQ772 A company runs container applications by using Amazon Elastic Kubernetes Service (Amazon EKS) and the\nKubernetes Horizontal Pod Autoscaler. The workload is not consistent throughout the day. A solutions architect notices\nthat the number of nodes does not automatically scale out when the existing nodes have reached maximum capacity in the\ncluster, which causes performance issues. Which solution will resolve this issue with the LEAST administrative overhead?\n\nA. Scale out the nodes by tracking the memory usage.\n\nB. Use the Kubernetes Cluster Autoscaler to manage the number of nodes in the cluster.\n\nc. Use an AWS Lambda function to resize the EKS cluster automatically.\n\nD. Use an Amazon EC2 Auto Scaling group to distribute the workload.\n", "image": "Screenshot 2024-12-27 at 12.04.34.png"}, {"text": "42a\n\nQ636 An ecommerce company wants a disaster recovery solution for its Amazon RDS DB instances that run Microsoft SQL\nServer Enterprise Edition.The company's current recovery point objective (RPO) and recovery time objective (RTO) are 24\nhours.Which solution will meet these requirements MOST cost-effectively?\n\nA. Create a cross-Region read replica and promote the read replica to the primary instance.\nB. Use AWS Database Migration Service (AWS DMS) to create RDS cross-Region replication.\n\nCc. Use cross-Region replication every 24 hours to copy native backups to an Amazon S3 bucket.\n\nD. Copy automatic snapshots to another Region every 24 hours.\n", "image": "Screenshot 2024-12-27 at 12.00.50.png"}, {"text": "440\n\nQ661 A company uses Amazon Elastic Kubernetes Service (Amazon EKS) to run a container application.The EKS cluster\nstores sensitive information in the Kubernetes secrets object. The company wants to ensure that the information is\nencrypted.Which solution will meet these requirements with the LEAST operational overhead?\n\nA. Use the container application to encrypt the information by using AWS Key Management Service (AWS KMS).\nB. Enable secrets encryption in the EKS cluster by using AWS Key Management Service (AWS KMS).\nCc. Implement an AWS Lambda function to encrypt the information by using AWS Key Management Service (AWS KMS).\n\nD. Use AWS Systems Manager Parameter Store to encrypt the information by using AWS Key Management Service\n(AWS KMS).\n", "image": "Screenshot 2024-12-27 at 12.01.04.png"}, {"text": "#105\n\nQ12 A company has created a multi-tier application for its ecommerce website.The website uses an Application Load\nBalancer that resides in the public subnets, a web tier in the public subnets, and a MySQL cluster hosted on Amazon EC2\ninstances in the private subnets.The MySQL database needs to retrieve product catalog and pricing information that is\nhosted on the internet by a third-party provider.A solutions architect must devise a strategy that maximizes security\nwithout increasing operational overhead.What should the solutions architect do to meet these requirements?\n\nA. Deploy a NAT instance in the VPC.Route all the internet-based trac through the NAT instance.\n\nB. Deploy a NAT gateway in the public subnets.Modify the private subnet route table to direct all internet-bound trac to\n\nthe NAT gateway.\n\nc. Congure an internet gateway and attach it to the VPModify the private subnet route table to direct internet-bound\ntrac to the internet gateway.\n\nD. Congure a virtual private gateway and attach it to the VPC.Modify the private subnet route table to direct internet-\nbound trac to the virtual private gateway.\n", "image": "Screenshot 2024-12-27 at 12.08.20.png"}, {"text": "#1038\n\nQ10 A company uses an Amazon DynamobB table to store data that the company receives from devices.The DynamobB\ntable supports acustomer-facing website to display recent activity on customer devices.The company configured the table\nwith provisioned throughput forwrites and reads.The company wants to calculate performance metrics for customer device\ndata on a dally basis.The solution must have minimal efect on table's provisioned read and write capacity.Which solution\nwill meet these requirements?\n\nA. Use an Amazon Athena SOL query with the Amazon Athena DynamobB connector to calculate performance metrics\non a recurringschedule.\n\nB. Use an AWS Glue job with the AWs Glue DynamobB export connector to calculate performance metrics on a recuring\n\nschedule.\n\nC. Use an Amazon Redshift COPY command to calculate performance metrics on a recurring schedule.\n\nD. Use an Amazon EIMR job with an Apache Hive external table to calculate performance metrics on a recurring\nschedule.\n", "image": "Screenshot 2024-12-27 at 12.08.09.png"}, {"text": "45a\n\nQ690 A company wants to run a gaming application on Amazon EC2 instances that are part of an Auto Scaling group in the\nAWS Cloud.The application will transmit data by using UDP packets.The company wants to ensure that the application can\nscale out and in as traffic increases and decreases.What should a solutions architect do to meet these requirements?\n\nA. Attach a Network Load Balancer to the Auto Scaling group.\nB. Attach an Application Load Balancer to the Auto Scaling group.\nCc. Deploy an Amazon Route 53 record set with a weighted policy to route traffic appropriately.\n\nD. Deploy a NAT instance that is configured with port forwarding to the EC2 instances in the Auto Scaling group.\n", "image": "Screenshot 2024-12-27 at 12.01.11.png"}, {"text": "135m\n\nQ1039 A company wants to migrate an application to AWS.The company wants to increase the application's current\navailability.The company wants to use AWS WAF in the application's architecture.Which solution will meet these\n\nrequirements?\n\nA. Create an Auto Scaling group that contains multiple Amazon EC2 instances that host the application across two\n\nAvailability Zones.Configure an Application Load Balancer (ALB) and set the Auto Scaling group as the\ntarget.Connect a WAF to the ALB.\n\nB. Create a cluster placement group that contains multiple Amazon EC2 instances that hosts the application.Configure\nan Application Load Balancer and set the EC2 instances as the targets.Connect a WAF to the placement group.\n\nCc. Create two Amazon EC2 instances that host the application across two Availability Zones.Configure the EC2\ninstances as the targets of an Application Load Balancer (ALB).Connect a WAF to the ALB.\n\nD. Create an Auto Scaling group that contains multiple Amazon EC2 instances that host the application across two\nAvailability Zones.Configure an Application Load Balancer (ALB) and set the Auto Scaling group as the\ntarget.Connect a WAF to the Auto Scaling group.\n", "image": "Screenshot 2024-12-27 at 12.11.28.png"}, {"text": "#1018\n\nQ8 A company that is in the ap-northeast-1 Region has a fleet of thousands of AWS Outposts servers.The company has\ndeployed the servers at remote locations around the world.All the servers regularly download new software versions that\nconsist of 100 files.There is significant latency before all servers run the new software versions.The company must reduce\nthe deployment latency for new software versions.Which solution will meet this requirement with the LEAST operational\n\noverhead?\n\nA. Create an Amazon S3 bucket in ap-northeast-1.Set up an Amazon CloudFront distribution in ap-northeast-1 that\nincludes a CachingDisabled cache policy.Configure the S3 bucket as the origin.Download the software by using\nsigned URLs.\n\nB. Create an Amazon S3 bucket in ap-northeast-1.Create a second S3 bucket in the us-east-1 Region.Configure\nreplication between the buckets.Set up an Amazon CloudFront distribution that uses ap-northeast-1 as the primary\norigin and us-east-1 as the secondary origin.Download the software by using signed URLs.\n\nCc. Create an Amazon S3 bucket in ap-northeast-1.Configure Amazon S3 Transfer Acceleration.Download the software\nby using the S3 Transfer Acceleration endpoint.\n\nD. Create an Amazon S3 bucket in ap-northeast-1.Set up an Amazon CloudFront distribution.Configure the S3 bucket\n\nas the origin.Download the software by using signed URLs.\n\n", "image": "Screenshot 2024-12-27 at 12.07.58.png"}, {"text": "53122\n\nQ29 A company is implementing a new application on AWS.The company will run the application on multiple Amazon EC2\ninstances across multiple Availability Zones within multiple AWS Regions.The application will be available through the\ninternet.Users will access the application from around the world.The company wants to ensure that each user who\naccesses the application is sent to the EC2 instances that are closest to the user's location.Which solution will meet these\nrequirements?\n\nA. Implement an Amazon Route 53 geolocation routing policy.Use an internet-facing Application Load Balancer to\ndistribute the trac across all Availability Zones within the same Region.\n\nB. Implement an Amazon Route 53 geoproximity routing policy.Use.an internet-facing Network Load Balancer to\n\ndistribute the trac across all Availability Zones within the same Region.\n\nCc. Implement an Amazon Route 53 multivalue answer routing policy.Use an internet-facing Application Load Balancer to\ndistribute the trac across all Availability Zones within the same Region.\n\nD. Implement an Amazon Route 53 weighted routing policy.Use an internet-facing Network Load Balancer to distribute\nthe trac across all Availability Zones within the same Region.\n", "image": "Screenshot 2024-12-27 at 12.09.49.png"}, {"text": "meom\n\nQ889 A company runs containers in a Kubernetes environment in the company's local data center. The company wants to\nuse Amazon Elastic Kubernetes Service (Amazon EKS) and other AWS managed services. Data must remain locally in the\ncompany's data center and cannot be stored in any remote site or cloud to maintain compliance. Which solution will meet\n\nthese requirements?\n\nA. Deploy AWS Local Zones in the company's data center.\n\nB. Use an AWS Snowmobile in the company's data center.\n\nCc. Install an AWS Outposts rack in the company's data center.\n\nD. Install an AWS Snowball Edge Storage Optimized node in the data center.\n", "image": "Screenshot 2024-12-27 at 12.06.32.png"}, {"text": "be 1528\n\nQ709 A company runs applications on AWS that connect to the company's Amazon RDS database.The applications scale\non weekends and at peak times of the year.The company wants to scale the database more effectively for its applications\nthat connect to the database.Which solution will meet these requirements with the LEAST operational overhead?\n\nA. Use Amazon DynamoDB with connection pooling with a target group configuration for the database.Change the\napplications to use the DynamoDB endpoint.\n\nB. Use Amazon RDS Proxy with a target group for the database.Change the applications to use the RDS Proxy endpoint.\n\nc. Use a custom proxy that runs on Amazon EC2 as an intermediary to the database.Change the applications to use the\ncustom proxy endpoint.\n\nD. Use an AWS Lambda function to provide connection pooling with a target group configuration for the\ndatabase.Change the applications to use the Lambda function.\n", "image": "Screenshot 2024-12-27 at 12.12.53.png"}, {"text": "840\n\nQ888 A company has an Amazon S3 data lake. The company needs a solution that transforms the data from the data lake\nand loads the data into a data warehouse every day. The data warehouse must have massively parallel processing (MPP)\ncapabilities. Data analysts then need to create and train machine learning (ML) models by using SQL commands on the\ndata. The solution must use serverless AWS services wherever possible. Which solution will meet these requirements?\n\nA. Run a daily Amazon EMR job to transform the data and load the data into Amazon Redshift. Use Amazon Redshift ML\nto create and train the ML models.\n\nB. Run a daily Amazon EMR job to transform the data and load the data into Amazon Aurora Serverless. Use Amazon\nAurora ML to create and train the ML models.\n\nc. Run a daily AWS Glue job to transform the data and load the data into Amazon Redshift Serverless. Use Amazon\n\nRedshift ML to create and train the ML models.\n\nD. Run a daily AWS Glue job to transform the data and load the data into Amazon Athena tables. Use Amazon Athena\nML to create and train the ML models.\n", "image": "Screenshot 2024-12-27 at 12.06.26.png"}, {"text": "S108\n\nQ15 A company hosts a video streaming web application ina VPC. The company uses a Network Load Balancer (NLB) to\nhandle TCP traffic for real-time data processing. There have been unauthorized attempts to access the application.The\ncompany wants to improve application security with minimal architectural change to prevent unauthorized attempts to\n\naccess the application.Which solution will meet these requirements?\n\nA. Implement a series of AWS WAF rules directly on the NLB to filter out unauthorized traffic.\n\nB. Recreate the NLB with a security group to allow only trusted IP addresses.\n\nC. Deploy a second NLB in parallel with the existing NLB configured with a strict IP address allow list.\n\nD. Use AWS Shield Advanced to provide enhanced DDoS protection and prevent unauthorized access attempts.\n", "image": "Screenshot 2024-12-27 at 12.08.36.png"}, {"text": "1308\n\nQ37 A company's image-hosting website gives users around the world the ability to up load, view, and download images\nfrom their mobile devices.The company currently hosts the static website in an Amazon S3 bucket.Because of the\nwebsite's growing popularity, the website's performance has decreased.Users have reported latency issues when they\nupload and download images.The company must improve the performance of the website.Which solution will meet these\nrequirements with the LEAST implementation effort?\n\nA. Configure an Amazon CloudFront distribution for the S3 bucket to improve the download performance.Enable S3\n\nTransfer Acceleration to improve the upload performance.\n\nB. Configure Amazon EC2 instances of the right sizes in multiple AWS Regions.Migrate the application to the EC2\ninstances.Use an Application Load Balancer to distribute the website traffic equally among the EC2\ninstances.Configure AWS Global Accelerator to address global demand with low latency.\n\nCc. Configure an Amazon CloudFront distribution that uses the S3 bucket as an origin to improve the download\nperformance.Configure the application to use CloudFront to upload images to improve the upload\nperformance.Create S3 buckets in multiple AWS Regions.Configure replication rules for the buckets to replicate\nusers' data based on the users' location.Redirect downloads to the S3 bucket that is closest to each user's location.\n\nD. Configure AWS Global Accelerator for the S3 bucket to improve network performance.Create an endpoint for the\napplication to use Global Accelerator instead of the S3 bucket.\n", "image": "Screenshot 2024-12-27 at 12.11.03.png"}, {"text": "63a\n\nQ768 A manufacturing company runs its report generation application on AWS. The application generates each report in\nabout 20 minutes. The application is built as a monolith that runs on a single Amazon EC2 instance. The application\nrequires frequent updates to its tightly coupled modules. The application becomes complex to maintain as the company\nadds new features. Each time the company patches a software module, the application experiences downtime. Report\ngeneration must restart from the beginning after any interruptions. The company wants to redesign the application so that\nthe application can be flexible, scalable, and gradually improved. The company wants to minimize application downtime.\nWhich solution will meet these requirements?\n\nA. Run the application on AWS Lambda as a single function with maximum provisioned concurrency.\n\nB. Run the application on Amazon EC2 Spot Instances as microservices with a Spot Fleet default allocation strategy.\n\nc. Run the application on Amazon Elastic Container Service (Amazon ECS) as microservices with service auto scaling.\n\nD. Run the application on AWS Elastic Beanstalk as a single application environment with an all-at-once deployment\nstrategy.\n", "image": "Screenshot 2024-12-27 at 12.04.22.png"}, {"text": "531268\n\nQ33 A financial services company plans to launch a new application on AWS to handle sensitive financial transactions.The\ncompany will deploy the application on Amazon EC2 instances.The company will use Amazon RDS for MySQL as the\ndatabase.The company's security policies mandate that data must be encrypted at rest and in transit.Which solution will\nmeet these requirements with the LEAST operational overhead?\n\nA. Configure encryption at rest for Amazon RDS for MySQL by using AWS KMS managed keys.Configure AWS\n\nCertificate Manager (ACM) SSL/TLS certificates for encryption in transit.\n\nB. Configure encryption at rest for Amazon RDS for MySQL by using AWS KMS managed keys.Configure IPsec tunnels\nfor encryption in transit.\n\nCc. Implement third-party application-level data encryption before storing data in Amazon RDS for MySQL.Configure\nAWS Certificate Manager (ACM) SSL/TLS certificates for encryption in transit.\n\nD. Configure encryption at rest for Amazon RDS for MySQL by using AWS KMS managed keys.Configure a VPN\nconnection to enable private connectivity to encrypt data in transit.\n", "image": "Screenshot 2024-12-27 at 12.10.43.png"}, {"text": "552080\n\nQ323 A company uses Amazon S3 as its data lake. The company has a new partner that must use SFTP to upload data\nfiles. A solutions architect needs to implement a highly available SFTP solution that minimizes operational overhead.Which\n\nsolution will meet these requirements?\n\nA. Use AWS Transfer Family to configure an SFTP-enabled server with a publicly accessible endpoint. Choose the S3\n\ndata lake as the destination.\n\nB. Use Amazon S3 File Gateway as an SFTP server. Expose the S3 File Gateway endpoint URL to the new partner. Share\nthe S3 File Gateway endpoint with the new partner.\n\nc. Launch an Amazon EC2 instance in a private subnet in a VPInstruct the new partner to upload files to the EC2\ninstance by using a VPN. Run a cron job script, on the EC2 instance to upload files to the S3 data lake.\n\nD. Launch Amazon EC2 instances in a private subnet in a VPC. Place a Network Load Balancer (NLB) in front of the EC2\ninstances. Create an SFTP listener port for the NLB. Share the NLB hostname with the new partner. Run a cron job\n\nscript on the EC2 instances to upload files to the S3 data lake.\n", "image": "Screenshot 2024-12-27 at 11.41.12.png"}, {"text": "#128\n\nQ35 A company runs database workloads on AWS that are the backend for the company's customer portals. The company\nruns a Multi-AZ database cluster on Amazon RDS for PostgreSQL.The company needs to implement a 30-day backup\nretention policy.The company currently has both automated RDS backups and manual RDS backups.The company wants to\nmaintain both types of existing RDS backups that are less than 30 days old.Which solution will meet these requirements\nMOST cost-effectively?\n\nA. Configure the RDS backup retention policy to 30 days for automated backups by using AWS Backup.Manually delete\nmanual backups that are older than 30 days.\n\nB. Disable RDS automated backups.Delete automated backups and manual backups that are older than 30\ndays.Configure the RDS backup retention policy to 30 days for automated backups.\n\nc. Configure the RDS backup retention policy to 30 days for automated backups.Manually delete manual backups that\nare older than 30 days.\n\nD. Disable RDS automated backups.Delete automated backups and manual backups that are older than 30 days\n\nautomatically by using AWS CloudFormation.Configure the RDS backup retention policy to 30 days for automated\nbackups.\n\n", "image": "Screenshot 2024-12-27 at 12.10.53.png"}, {"text": "43a\n\nQ657 A company is building a data analysis platform on AWS by using AWS Lake Formation.The platform will ingest data\nfrom different sources such as Amazon S3 and Amazon RDS.The company needs a secure solution to prevent access to\nportions of the data that contain sensitive information.Which solution will meet these requirements with the LEAST\noperational overhead?\n\nA. Create an IAM role that includes permissions to access Lake Formation tables.\nB. Create data filters to implement row-level security and cell-level security.\nCc. Create an AWS Lambda function that removes sensitive information before Lake Formation ingests the data.\n\nD. Create an AWS Lambda function that periodically queries and removes sensitive information from Lake Formation\ntables.\n", "image": "Screenshot 2024-12-27 at 12.00.56.png"}, {"text": "Bsa\n\nQ310 A company has a small Python application that processes JSON documents and outputs the results to an on-\npremises SQL database. The application runs thousands of times each day. The company wants to move the application to\nthe AWS Cloud. The company needs a highly available solution that maximizes scalability and minimizes operational\noverhead.Which solution will meet these requirements?\n\nA. Place the JSON documents in an Amazon S3 bucket. Run the Python code on multiple Amazon EC2 instances to\nprocess the documents. Store the results in an Amazon Aurora DB cluster.\n\nB. Place the JSON documents in an Amazon S3 bucket. Create an AWS Lambda function that runs the Python code to\n\nprocess the documents as they arrive in the S3 bucket. Store the results in an Amazon Aurora DB cluster.\n\nc. Place the JSON documents in an Amazon Elastic Block Store (Amazon EBS) volume. Use the EBS Multi-Attach\nfeature to attach the volume to multiple Amazon EC2 instances. Run the Python code on the EC2 instances to\nprocess the documents. Store the results on an Amazon RDS DB instance.\n\nD. Place the JSON documents in an Amazon Simple Queue Service (Amazon SQS) queue as messages. Deploy the\nPython code as a container on an Amazon Elastic Container Service (Amazon ECS) cluster that is configured with the\nAmazon EC2 launch type. Use the container to process the SQS messages. Store the results on an Amazon RDS DB\ninstance.\n", "image": "Screenshot 2024-12-27 at 11.40.56.png"}, {"text": "178\nQ308 A company wants to run a hybrid workload for data processing.The data needs to be accessed by on-premises\napplications for local data processing using an NFS protocol, and must also be accessible from the AWS Cloud for further\n\nanalytics and batch processing.Which solution will meet these requirements?\n\nA. Use an AWS Storage Gateway file gateway to provide file storage to AWS, then perform analytics on this data in the\n\nAWS Cloud.\n\nB. Use an AWS storage Gateway tape gateway to copy the backup of the local data to AWS, then perform analytics on\nthis data in the AWS cloud.\n\nc.Use an AWS Storage Gateway volume gateway in a stored volume configuration to regularly take snapshots of the\nlocal data, then copy the data to AWS.\n\nD. Use an AWS Storage Gateway volume gateway in a cached volume configuration to back up all the local storage in\nthe AWS cloud, then perform analytics on this data in the cloud.\n", "image": "Screenshot 2024-12-27 at 11.40.42.png"}, {"text": "132m\n\nQ39 A company is migrating an application from an on-premises location to Amazon Elastic Kubernetes Service (Amazon\nEKS).The company must use a custom subnet for pods that are in the company's VPC to comply with requirements.The\ncompany also needs to ensure that the pods can communicate securely within the pods' VPC.Which solution will meet\n\nthese requirements?\n\nA. Congure AWS Transit Gateway to directly manage custom subnet congurations for the pods in Amazon EKS.\nB. Create an AWS Direct Connect connection from the company's on-premises IP address ranges to the EKS pods.\n\nc. Use the Amazon VPC CNI plugin for Kubernetes.Dene custom subnets in the VPC cluster for the pods to use.\n\nD. Implement a Kubernetes network policy that has pod anti-anity rules to restrict pod placement to specic nodes that\nare within custom subnets.\n", "image": "Screenshot 2024-12-27 at 12.11.13.png"}, {"text": "#150\n\nQ1001 A company operates a food delivery service.Because of recent growth, the company's order processing system is\nexperiencing scaling problems during peak traffic hours.The current architecture includes Amazon EC2 instances in an\nAuto Scaling group that collect orders from an application.A second group of EC2 instances in an Auto Scaling group fulfills\nthe orders.The order collection process occurs quickly, but the order fulfillment process can take longer.Data must not be\nlost because of a scaling event.A solutions architect must ensure that the order collection process and the order fulfillment\nprocess can both scale adequately during peak traffic hours.Which solution will meet these requirements?\n\nA. Use Amazon CloudWatch to monitor the CPUUtilization metric for each instance in both Auto Scaling\ngroups.Configure each Auto Scaling group's minimum capacity to meet its peak workload value.\n\nB. Use Amazon CloudWatch to monitor the CPUUtilization metric for each instance in both Auto Scaling\ngroups.Configure a CloudWatch alarm to invoke an Amazon Simple Notification Service (Amazon SNS) topic to\ncreate additional Auto Scaling groups on demand.\n\nC. Provision two Amazon Simple Queue Service (Amazon SQS) queues.Use one SQS queue for order collection.Use the\nsecond SQS queue for order fulfillment.Configure the EC2 instances to poll their respective queues.Scale the Auto\nScaling groups based on notifications that the queues send.\n\nD. Provision two Amazon Simple Queue Service (Amazon SQS) queues.Use one SQS queue for order collection.Use the\n\nsecond SQS queue for order fulfillment.Configure the EC2 instances to poll their respective queues.Scale the Auto\nScaling groups based on the number of messages in each queue.\n\n", "image": "Screenshot 2024-12-27 at 12.12.42.png"}, {"text": "S068\n\nQ13 A company is migrating from a monolithic architecture for a web application that is hosted on Amazon EC2 to a\nserverless microservices architecture. The company wants to use AWS services that support an event-driven, loosely\ncoupled architecture. The company wants to use the publish/subscribe (pub/sub) pattern. Which solution will meet these\n\nrequirements MOST cost-effectively?\n\nA. Configure an Amazon API Gateway REST API to invoke an AWS Lambda function that publishes events to an Amazon\nSimple Queue Service (Amazon SQS) queue. Configure one or more subscribers to read events from the SQS queue.\n\nB. Configure an Amazon API Gateway REST API to invoke an AWS Lambda function that publishes events to an Amazon\nSimple Notification Service (Amazon SNS) topic. Configure one or more subscribers to receive events from the SNS\ntopic.\n\nCc. Configure an Amazon API Gateway WebSocket API to write to a data stream in Amazon Kinesis Data Streams with\nenhanced fan-out. Configure one or more subscribers to receive events from the data stream.\n\nD. Configure an Amazon API Gateway HTTP API to invoke an AWS Lambda function that publishes events to an Amazon\n\nSimple Notification Service (Amazon SNS) topic. Configure one or more subscribers to receive events from the topic.\n\n", "image": "Screenshot 2024-12-27 at 12.08.26.png"}, {"text": "Ey 2\n\nQ691 A company runs several websites on AWS for its different brands.Each website generates tens of gigabytes of web\ntraffic logs each day.A solutions architect needs to design a scalable solution to give the company's developers the ability\nto analyze traffic patterns across all the company's websites.This analysis by the developers will occur on demand once a\nweek over the course of several months.The solution must support queries with standard SQL.Which solution will meet\nthese requirements MOST cost-effectively?\n\nA. Store the logs in Amazon S3.Use Amazon Athena tor analysis.\n\nB. Store the logs in Amazon RDS.Use a database client for analysis.\nCc. Store the logs in Amazon OpenSearch Service.Use OpenSearch Service for analysis.\n\nD. Store the logs in an Amazon EMR cluster Use a supported open-source framework for SQL-based analysis.\n", "image": "Screenshot 2024-12-27 at 12.01.17.png"}, {"text": "5B41\n\nQ611 A company needs to integrate with a third-party data feed.The data feed sends a webhook to notify an external\nservice when new data is ready for consumption.A developer wrote an AWS Lambda function to retrieve data when the\ncompany receives a webhook callback.The developer must make the Lambda function available for the third party to\ncall.Which solution will meet these requirements with the MOST operational efficiency?\n\nA. Create a function URL for the Lambda function.Provide the Lambda function URL to the third party for the webhook.\n\nB. Deploy an Application Load Balancer (ALB) in front of the Lambda function.Provide the ALB URL to the third party for\nthe webhook.\n\nc. Create an Amazon Simple Notification Service (Amazon SNS) topic.Attach the topic to the Lambda function.Provide\nthe public hostname of the SNS topic to the third party for the webhook.\n\nD. Create an Amazon Simple Queue Service (Amazon SQS) queue.Attach the queue to the Lambda function.Provide the\npublic hostname of the SQS queue to the third party for the webhook.\n", "image": "Screenshot 2024-12-27 at 12.00.43.png"}, {"text": "1\n\nQ2 A company is developing a two-tier web application on AWS.The company's developers have deployed the application\non an Amazon EC2 instance that connects directly to a backend Amazon RDS database.The company must not hardcode\ndatabase credentials in the application.The company must also implement a solution to automatically rotate the database\ncredentials on a regular basis.Which solution will meet these requirements with the LEAST operational overhead?\n\nA. Store the database credentials in the instance metadata.Use Amazon EventBridge (Amazon CloudWatch Events)\nrules to run a scheduled AWS Lambda function that updates the RDS credentials and instance metadata at the same\ntime.\n\nB. Store the database credentials in a configuration file in an encrypted Amazon S3 bucket.Use Amazon EventBridge\n(Amazon CloudWatch Events) rules to run a scheduled AWS Lambda function that updates the RDS credentials and\nthe credentials in the configuration file at the same time.Use S3 Versioning to ensure the ability to fall back to\nprevious values.\n\nCc. Store the database credentials as a secret in AWS Secrets Manager.Turn on automatic rotation for the secret.Attach\n\nthe required permission to the EC2 role to grant access to the secret.\n\nD. Store the database credentials as encrypted parameters in AWS Systems Manager Parameter Store.Turn on\nautomatic rotation for the encrypted parameters.Attach the required permission to the EC2 role to grant access to\nthe encrypted parameters.\n", "image": "Screenshot 2024-12-27 at 11.35.19.png"}, {"text": "oom\n\nQ6 A solutions architect needs to connect a company's corporate network to its VPC to allow on-premises access to its\nAWS resources.The solution must provide encryption of all traffic between the corporate network and the VPC at the\nnetwork layer and the session layer.The solution also must provide security controls to prevent unrestricted access\nbetween AWS and the on-premises systems.Which solution meets these requirements?\n\nA. Configure AWS Direct Connect to connect to the VPC.Configure the VPC route tables to allow and deny traffic\nbetween AWS and on premises as required.\n\nB. Create an IAM policy to allow access to the AWS Management Console only from a defined set of corporate IP\naddresses.Restrict user access based on job responsibility by using an IAM policy and roles.\n\nC. Configure AWS Site-to-Site VPN to connect to the VPConfigure route table entries to direct traffic from on premises\n\nto the VPConfigure instance security groups and network ACLs to allow only required traffic from on premises.\n\nD. Configure AWS Transit Gateway to connect to the VPC.Configure route table entries to direct traffic from on\npremises to the VPC.Configure instance security groups and network ACLs to allow only required traffic from on\npremises.\n", "image": "Screenshot 2024-12-27 at 12.07.48.png"}, {"text": "21378\n\nQ1036 A company is migrating five on-premises applications to VPCs in the AWS Cloud.Each application is currently\ndeployed in isolated virtual networks on premises and should be deployed similarly in the AWS Cloud.The applications\nneed to reach a shared services VPC.AIl the applications must be able to communicate with each other.If the migration is\nsuccessful, the company will repeat the migration process for more than 100 applications.Which solution will meet these\nrequirements with the LEAST administrative overhead?\n\nA. Deploy software VPN tunnels between the application VPCs and the shared services VPC.Add routes between the\napplication VPCs in their subnets to the shared services VPC.\n\nB. Deploy VPC peering connections between the application VPCs and the shared services VPC.Add routes between\nthe application VPCs in their subnets to the shared services VPC through the peering connection.\n\nC. Deploy an AWS Direct Connect connection between the application VPCs and the shared services VPAdd routes\nfrom the application VPCs in their subnets to the shared services VPC and the applications VPCs.Add routes from\nthe shared services VPC subnets to the applications VPCs.\n\nD. Deploy a transit gateway with associations between the transit gateway and the application VPCs and the shared\n\nservices VPC.Add routes between the application VPCs in their subnets and the application VPCs to the shared\nservices VPC through the transit gateway.\n\n", "image": "Screenshot 2024-12-27 at 12.11.38.png"}, {"text": "51074\n\nQ14 A company needs to retain its AWS CloudTrail logs for 3 years. The company is enforcing CloudTrail across a set of\nAWS accounts by using AWS Organizations from the parent account. The CloudTrail target S3 bucket is configured with S3\nVersioning enabled. An S3 Lifecycle policy is in place to delete current objects after 3 years.After the fourth year of use of\nthe S3 bucket, the S3 bucket metrics show that the number of objects has continued to rise. However, the number of new\nCloudTrail logs that are delivered to the S3 bucket has remained consistent.Which solution will delete objects that are older\n\nthan 3 years in the MOST cost-effective manner?\n\nA. Configure the organization's centralized CloudTrail trail to expire objects after 3 years.\n\nB. Configure the S3 Lifecycle policy to delete previous versions as well as current versions.\n\nC. Create an AWS Lambda function to enumerate and delete objects from Amazon S3 that are older than 3 years.\n\nD. Configure the parent account as the owner of all objects that are delivered to the S3 bucket.\n", "image": "Screenshot 2024-12-27 at 12.08.31.png"}, {"text": "830i\n\nQ887 A solutions architect is designing a user authentication solution for a company. The solution must invoke two-factor\nauthentication for users that log in from inconsistent geographical locations, IP addresses, or devices. The solution must\nalso be able to scale up to accommodate millions of users. Which solution will meet these requirements?\n\nA. Configure Amazon Cognito user pools for user authentication. Enable the risk-based adaptive authentication feature\n\nwith multifactor authentication (MFA).\n\nB. Configure Amazon Cognito identity pools for user authentication. Enable multi-factor authentication (MFA).\n\nc. Configure AWS Identity and Access Management (IAM) users for user authentication. Attach an IAM policy that\nallows the AllowManageOwnUserMFA action.\n\nD. Configure AWS IAM Identity Center (AWS Single Sign-On) authentication for user authentication. Configure the\npermission sets to require multi-factor authentication (MFA).\n", "image": "Screenshot 2024-12-27 at 12.06.21.png"}, {"text": "Boom\n\nQ745 A solutions architect creates a VPC that includes two public subnets and two private subnets. A corporate security\nmandate requires the solutions architect to launch all Amazon EC2 instances in a private subnet. However, when the\nsolutions architect launches an EC2 instance that runs a web server on ports 80 and 443 in a private subnet, no external\ninternet traffic can connect to the server. What should the solutions architect do to resolve this issue?\n\nA. Attach the EC2 instance to an Auto Scaling group in a private subnet. Ensure that the DNS record for the website\nresolves to the Auto Scaling group identifier.\n\nB. Provision an internet-facing Application Load Balancer (ALB) in a public subnet. Add the EC2 instance to the target\n\ngroup that is associated with the ALEnsure that the DNS record for the website resolves to the ALB.\n\nc. Launch a NAT gateway in a private subnet. Update the route table for the private subnets to add a default route to\nthe NAT gateway. Attach a public Elastic IP address to the NAT gateway.\n\nD. Ensure that the security group that is attached to the EC2 instance allows HTTP traffic on port 80 and HTTPS traffic\non port 443. Ensure that the DNS record for the website resolves to the public IP address of the EC2 instance.\n", "image": "Screenshot 2024-12-27 at 12.03.39.png"}, {"text": "8220\n\nQ382 An ecommerce company stores terabytes of customer data in the AWS Cloud.The data contains personally\nidentifiable information (Pll). The company wants to use the data in three applications.Only one of the applications needs to\nprocess the PIl.The Pll must be removed before the other two applications process the data.Which solution will meet these\nrequirements with the LEAST operational overhead?\n\nA. Store the data in an Amazon DynamoDB table.Create a proxy application layer to intercept and process the data that\neach application requests.\n\nB. Store the data in an Amazon S3 bucket.Process and transform the data by using S3 Object Lambda before returning\n\nthe data to the requesting application.\n\nCc. Process the data and store the transformed data in three separate Amazon S3 buckets so that each application has\nits own custom dataset.Point each application to its respective S3 bucket.\n\nD. Process the data and store the transformed data in three separate Amazon DynamoDB tables so that each\napplication has its own custom dataset.Point each application to its respective DynamoDB table.\n", "image": "Screenshot 2024-12-27 at 11.41.28.png"}, {"text": "2H\n\nQ38 A company needs to store data in Amazon S3 and must prevent the data from being changed.The company wants\nnew objects that are uploaded to Amazon S3 to remain unchangeable for a nonspecific amount of time until the company\ndecides to modify the objects.Only specific users in the company's AWS account can have the ability to delete the\nobjects.What should a solutions architect do to meet these requirements?\n\nA. Create an S3 Glacier vault Apply a write-once, read-many (WORM) vault lock policy to the objects.\n\nB. Create an S3 bucket with S3 Object Lock enabled Enable versioning.Set a retention period of 100 years.Use\ngovernance mode as the S3 bucket's default retention mode for new objects.\n\nCc. Create an S3 bucket.Use AWS CloudTrail to track any S3 API events that modify the objects.Upon notification,\nrestore the modified objects from any backup versions that the company has.\n\nD. Create an S3 bucket with S3 Object Lock enabled.Enable versioning.Add a legal hold to the objects.Add the\n\ns3:PutObjectLegalHold permission to the IAM policies of users who need to delete the objects.\n\n", "image": "Screenshot 2024-12-27 at 11.35.30.png"}, {"text": "oom\n\nQ754 A company hosts a database that runs on an Amazon RDS instance that is deployed to multiple Availability Zones.\nThe company periodically runs a script against the database to report new entries that are added to the database. The\nscript that runs against the database negatively affects the performance of a critical application. The company needs to\nimprove application performance with minimal costs. Which solution will meet these requirements with the LEAST\noperational overhead?\n\nA. Add functionality to the script to identify the instance that has the fewest active connections. Configure the script to\nread from that instance to report the total new entries.\n\nB. Create a read replica of the database. Configure the script to query only the read replica to report the total new\n\nentries.\n\nCc. Instruct the development team to manually export the new entries for the day in the database at the end of each day.\n\nD. Use Amazon ElastiCache to cache the common queries that the script runs against the database.\n", "image": "Screenshot 2024-12-27 at 12.04.01.png"}, {"text": "131g\n\nQ38 An online gaming company is transitioning user data storage to Amazon DynamoDB to support the company's\ngrowing user base.The current architecture includes DynamoDB tables that contain user profiles, achievements, and in-\ngame transactions.The company needs to design a robust, continuously available, and resilient DynamoDB architecture to\nmaintain a seamless gaming experience for users.Which solution will meet these requirements MOST cost-effectively?\n\nA. Create DynamoDB tables in a single AWS Region.Use on-demand capacity mode.Use global tables to replicate data\nacross multiple Regions.\n\nB. Use DynamoDB Accelerator (DAX) to cache frequently accessed data.Deploy tables in a single AWS Region and\nenable auto scaling.Configure Cross-Region Replication manually to additional Regions.\n\nCc. Create DynamoDB tables in multiple AWS Regions.Use on-demand capacity mode.Use DynamoDB Streams for\nCross-Region Replication between Regions.\n\nD. Use DynamoDB global tables for automatic multi-Region replication.Deploy tables in multiple AWS Regions.Use\n\nprovisioned capacity mode.Enable auto scaling.\n\n", "image": "Screenshot 2024-12-27 at 12.11.08.png"}, {"text": "512K\n\nQ203 A company is hosting a static website on Amazon S3 and is using Amazon Route 53 for DNS.The website is\nexperiencing increased demand from around the world.The company must decrease latency for users who access the\nwebsite.Which solution meets these requirements MOST cost-effectively?\n\nA. Replicate the S3 bucket that contains the website to all AWS Regions.Add Route 53 geolocation routing entries.\n\nB. Provision accelerators in AWS Global Accelerator.Associate the supplied IP addresses with the S3 bucket.Edit the\nRoute 53 entries to point to the IP addresses of the accelerators.\n\nc. Add an Amazon CloudFront distribution in front of the S3. bucket.Edit the Route 53 entries to point to the CloudFront\n\ndistribution.\n\nD. Enable S3 Transfer Acceleration on the bucket.Edit the Route 53 entries to point to the new endpoint.\n", "image": "Screenshot 2024-12-27 at 11.37.04.png"}, {"text": "sow\nQ890 A social media company has workloads that collect and process data. The workloads store the data in on-premises\n\nNFS storage. The data store cannot scale fast enough to meet the company's expanding business needs. The company\nwants to migrate the current data store to AWS. Which solution will meet these requirements MOST cost-effectively?\n\nA. Set up an AWS Storage Gateway Volume Gateway. Use an Amazon S3 Lifecycle policy to transition the data to the\nappropriate storage class.\n\nB. Set up an AWS Storage Gateway Amazon S3 File Gateway. Use an Amazon S3 Lifecycle policy to transition the data\n\nto the appropriate storage class.\n\nc. Use the Amazon Elastic File System (Amazon EFS) Standard-Infrequent Access (Standard-lA) storage class. Activate\nthe infrequent access lifecycle policy.\n\nD. Use the Amazon Elastic File System (Amazon EFS) One Zone-Infrequent Access (One Zone-IA) storage class.\nActivate the infrequent access lifecycle policy.\n", "image": "Screenshot 2024-12-27 at 12.06.38.png"}, {"text": "B81G\n\nQ845 A company uses AWS to run its ecommerce platform. The platform is critical to the company's operations and has a\nhigh volume of traffic and transactions. The company configures a multi-factor authentication (MFA) device to secure its\nAWS account root user credentials. The company wants to ensure that it will not lose access to the root user account if the\nMFA device is lost. Which solution will meet these requirements?\n\nA. Set up a backup administrator account that the company can use to log in if the company loses the MFA device.\n\nB. Add multiple MFA devices for the root user account to handle the disaster scenario.\n\nC. Create a new administrator account when the company cannot access the root account.\n\nD. Attach the administrator policy to another IAM user when the company cannot access the root account.\n", "image": "Screenshot 2024-12-27 at 12.06.10.png"}, {"text": "#1048\n\nQ11 A company recently performed a lift and shift migration of its on-premises Oracle database workload to run on an\nAmazon EC2 memory optimized Linux instance.The EC2 Linux instance uses a 1 TB Provisioned IOPS SSD (io1) EBS volume\nwith 64,000 IOPS.The database storage performance after the migration is slower than the performance of the on-\npremises database.Which solution will improve storage performance?\n\nA. Add more Provisioned IOPS SSD (i01) EBS volumes.Use OS commands to create a Logical Volume Management\n\n(LVM) stripe.\n\nB. Increase the Provisioned IOPS SSD (io1) EBS volume to more than 64,000 IOPS.\n\nCc. Increase the size of the Provisioned IOPS SSD (io1) EBS volume to 2 TB.\n\nD. Change the EC2 Linux instance to a storage optimized instance type.Do not change the Provisioned IOPS SSD (io1)\nEBS volume.\n", "image": "Screenshot 2024-12-27 at 12.08.14.png"}, {"text": "48a\n\nQ729 A company uses Amazon EC2 instances and stores data on Amazon Elastic Block Store (Amazon EBS) volumes.The\ncompany must ensure that all data is encrypted at rest by using AWS Key Management Service (AWS KMS).The company\nmust be able to control rotation of the encryption keys.Which solution will meet these requirements with the LEAST\n\noperational overhead?\n\nA. Create a customer managed key.Use the key to encrypt the EBS volumes.\n\nB. Use an AWS managed key to encrypt the EBS volumes.Use the key to configure automatic key rotation.\nCc. Create an external KMS key with imported key material.Use the key to encrypt the EBS volumes.\n\nD. Use an AWS owned key to encrypt the EBS volumes.\n", "image": "Screenshot 2024-12-27 at 12.01.30.png"}, {"text": "55138\n\nQ222 An online retail company has more than 50 million active customers and receives more than 25,000 orders each\nday.The company collects purchase data for customers and stores this data in Amazon S3.Additional customer data is\nstored in Amazon RDS.The company wants to make all the data available to various teams so that the teams can perform\nanalytics.The solution must provide the ability to manage fine-grained permissions for the data and must minimize\noperational overhead.Which solution will meet these requirements?\n\nA. Migrate the purchase data to write directly to Amazon RDS.Use RDS access controls to limit access.\n\nB. Schedule an AWS Lambda function to periodically copy data from Amazon RDS to Amazon S3.Create an AWS Glue\ncrawler.Use Amazon Athena to query the data.Use S3 policies to limit access.\n\nCc. Create a data lake by using AWS Lake Formation.Create an AWS Glue JDBC connection to Amazon RDS.Register (he\n\nS3 bucket in Lake Formation.Use Lake Formation access controls to limit access.\n\nD. Create an Amazon Redshift cluster.Schedule an AWS Lambda function to periodically copy data from Amazon S3\nand Amazon RDS to Amazon Redshift.Use Amazon Redshift access controls to limit access.\n", "image": "Screenshot 2024-12-27 at 11.37.11.png"}, {"text": "S768\n\nQ828 A company has hired an external vendor to perform work in the company\u2019s AWS account. The vendor uses an\nautomated tool that is hosted in an AWS account that the vendor owns. The vendor does not have IAM access to the\ncompany\u2019s AWS account. The company needs to grant the vendor access to the company's AWS account. Which solution\n\nwill meet these requirements MOST securely?\n\nA. Create an IAM role in the company\u2019s account to delegate access to the vendor's IAM role. Attach the appropriate IAM\n\npolicies to the role for the permissions that the vendor requires.\n\nB. Create an IAM user in the company's account with a password that meets the password complexity requirements.\nAttach the appropriate IAM policies to the user for the permissions that the vendor requires.\n\nC. Create an IAM group in the company's account. Add the automated tool's IAM user from the vendor account to the\ngroup. Attach the appropriate IAM policies to the group for the permissions that the vendor requires.\n\nD. Create an IAM user in the company's account that has a permission boundary that allows the vendor's account.\nAttach the appropriate IAM policies to the user for the permissions that the vendor requires.\n", "image": "Screenshot 2024-12-27 at 12.05.40.png"}, {"text": "eam\n\nQ770 A company has an AWS Direct Connect connection from its on-premises location to an AWS account. The AWS\naccount has 30 different VPCs in the same AWS Region. The VPCs use private virtual interfaces (VIFs). Each VPC has a\nCIDR block that does not overlap with other networks under the company's control. The company wants to centrally\nmanage the networking architecture while still allowing each VPC to communicate with all other VPCs and on-premises\nnetworks. Which solution will meet these requirements with the LEAST amount of operational overhead?\n\nA. Create a transit gateway, and associate the Direct Connect connection with a new transit VIF. Turn on the transit\n\ngateway's route propagation feature.\n\nB. Create a Direct Connect gateway. Recreate the private VIFs to use the new gateway. Associate each VPC by creating\nnew virtual private gateways.\n\nC. Create a transit VPConnect the Direct Connect connection to the transit VPCreate a peering connection between all\nother VPCs in the Region. Update the route tables.\n\nD. Create AWS Site-to-Site VPN connections from on premises to each VPC. Ensure that both VPN tunnels are UP for\neach connection. Turn on the route propagation feature.\n", "image": "Screenshot 2024-12-27 at 12.04.28.png"}, {"text": "#520\n\nQ738 A company regularly uploads GB-sized files to Amazon S3. After the company uploads the files, the company uses a\nfleet of Amazon EC2 Spot Instances to transcode the file format. The company needs to scale throughput when the\ncompany uploads data from the on-premises data center to Amazon S3 and when the company downloads data from\nAmazon S3 to the EC2 instances. Which solutions will meet these requirements? (Choose two.)\n\nA. Use the S3 bucket access point instead of accessing the S3 bucket directly.\nB. Upload the files into multiple S3 buckets.\nc. Use S3 multipart uploads.\n\nD. Fetch multiple byte-ranges of an object in parallel:\n\nE. Add a random prefix to each object when uploading the files.\n", "image": "Screenshot 2024-12-27 at 12.03.21.png"}, {"text": "#1278\n\nQ34 A company tracks customer satisfaction by using surveys that the company hosts on its website.The surveys\nsometimes reach thousands of customers every hour.Survey results are currently sent in email messages to the company\nso company employees can manually review results and assess customer sentiment.The company wants to automate the\ncustomer survey process.Survey results must be available for the previous 12 months.Which solution will meet these\nrequirements in the MOST scalable way?\n\nA. Send the survey results data to an Amazon API Gateway endpoint that is connected to an Amazon Simple Queue\nService (Amazon SQS) queue.Create an AWS Lambda function to poll the SQS queue, call Amazon Comprehend for\n\nsentiment analysis, and save the results to an Amazon DynamoDB table.Set the TTL for all records to 365 days in the\nfuture.\n\nB. Send the survey results data to an API that is running on an Amazon EC2 instance.Configure the API to store the\nsurvey results as a new record in an Amazon DynamoDB table, call Amazon Comprehend for sentiment analysis, and\nsave the results in a second DynamoDB table.Set the TTL for all records to 365 days in the future.\n\nC. Write the survey results data to an Amazon S3 bucket.Use S3 Event Notifications to invoke an AWS Lambda function\nto read the data and call Amazon Rekognition for sentiment analysis.Store the sentiment analysis results in a second\nS3 bucket.Use S3 lifecycle policies on each bucket to expire objects after 365 days.\n\nD. Send the survey results data to an Amazon API Gateway endpoint that is connected to an Amazon Simple Queue\nService (Amazon SQS) queue.Configure the SQS queue to invoke an AWS Lambda function that calls Amazon Lex for\nsentiment analysis and saves the results to an Amazon DynamoDB table.Set the TTL for all records to 365 days in\nthe future.\n", "image": "Screenshot 2024-12-27 at 12.10.49.png"}, {"text": "B62\n\nQ759 An ecommerce company runs its application on AWS. The application uses an Amazon Aurora PostgreSQL cluster in\nMulti-AZ mode for the underlying database. During a recent promotional campaign, the application experienced heavy\nread load and write load. Users experienced timeout issues when they attempted to access the application. A solutions\narchitect needs to make the application architecture more scalable and highly available. Which solution will meet these\nrequirements with the LEAST downtime?\n\nA. Create an Amazon EventBridge rule that has the Aurora cluster as a source. Create an AWS Lambda function to log\nthe state change events of the Aurora cluster. Add the Lambda function as a target for the EventBridge rule. Add\nadditional reader nodes to fail over to.\n\nB. Modify the Aurora cluster and activate the zero-downtime restart (ZDR) feature. Use Database Activity Streams on\nthe cluster to track the cluster status.\n\nc. Add additional reader instances to the Aurora cluster. Create an Amazon RDS Proxy target group for the Aurora\n\ncluster.\n\nD. Create an Amazon ElastiCache for Redis cache. Replicate data from the Aurora cluster to Redis by using AWS\nDatabase Migration Service (AWS DMS) with a write-around approach.\n", "image": "Screenshot 2024-12-27 at 12.04.16.png"}, {"text": "51082\n\nQ152 A company is developing an ecommerce application that will consist of a load-balanced front end, a container-based\napplication, and a relational database.A solutions architect needs to create a highly available solution that operates with as\nlittle manual intervention as possible.Which solutions meet these requirements? (Choose two.)\n\nA. Create an Amazon RDS DB instance in Multi-AZ mode.\n\nB. Create an Amazon RDS DB instance and one or more replicas in another Availability Zone.\n\nc. Create an Amazon EC2 instance-based Docker cluster to handle the dynamic application load.\n\nD. Create an Amazon Elastic Container Service (Amazon ECS).cluster with a Fargate launch type to handle the dynamic\n\napplication load.\n\nE. Create an Amazon Elastic Container Service (Amazon ECS) cluster with an Amazon EC2 launch type to handle the\ndynamic application load.\n", "image": "Screenshot 2024-12-27 at 11.36.47.png"}, {"text": "#1008\n\nQ7 A company has an employee web portal.Employees log in to the portal to view payroll details. The company is\ndeveloping a new system to give employees the ability to upload scanned documents for reimbursement.The company\nruns a program to extract text-based data from the documents and attach the extracted information to each employee's\nreimbursement IDs for processing.The employee web portal requires 100% uptime.The document extract program runs\ninfrequently throughout the day on an on-demand basis.The company wants to build a scalable and cost-effective new\nsystem that will require minimal changes to the existing web portal.The company does not want to make any code\nchanges.Which solution will meet these requirements with the LEAST implementation effort?\n\nA. Run Amazon EC2 On-Demand Instances in an Auto Scaling group for the web portal.Use an AWS Lambda function to\n\nrun the document extract program.Invoke the Lambda function when an employee uploads a new reimbursement\ndocument.\n\nB. Run Amazon EC2 Spot Instances in an Auto Scaling group for the web portal.Run the document extract program on\nEC2 Spot Instances.Start document extract program instances when an employee uploads a new reimbursement\ndocument.\n\nC. Purchase a Savings Plan to run the web portal and the document extract program.Run the web portal and the\ndocument extract program in an Auto Scaling group.\n\nD. Create an Amazon S3 bucket to host the web portal.Use Amazon API Gateway and an AWS Lambda function for the\nexisting functionalities. Use the Lambda function to run the document extract program.Invoke the Lambda function\nwhen the API that is associated with a new document upload is called.\n", "image": "Screenshot 2024-12-27 at 12.07.53.png"}, {"text": "31340\n\nQ1040 A company manages a data lake in an Amazon S3 bucket that numerous applications access.The S3 bucket\ncontains a unique prefix for each application. The company wants to restrict each application to its specific prefix and to\nhave granular control of the objects under each prefix.Which solution will meet these requirements with the LEAST\n\noperational overhead?\n\nA. Create dedicated S3 access points and access point policies for each application.\n\nB. Create an S3 Batch Operations job to set the ACL permissions for each object in the S3 bucket.\nCc. Replicate the objects in the S3 bucket to new S3 buckets for each application.Create replication rules by prefix.\n\nD. Replicate the objects in the S3 bucket to new S3 buckets for each application.Create dedicated S3 access points for\neach application.\n", "image": "Screenshot 2024-12-27 at 12.11.23.png"}, {"text": "5798\n\nQ839 A company has AWS Lambda functions that use environment variables. The company does not want its developers\nto see environment variables in plaintext. Which solution will meet these requirements?\n\nA. Deploy code to Amazon EC2 instances instead of using Lambda functions.\n\nB. Configure SSL encryption on the Lambda functions to use AWS CloudHSM to store and encrypt the environment\nvariables.\n\nC. Create a certificate in AWS Certificate Manager (ACM). Configure the Lambda functions to use the certificate to\nencrypt the environment variables.\n\nD. Create an AWS Key Management Service (AWS KMS) key. Enable encryption helpers on the Lambda functions to use\n\nthe KMS key to store and encrypt the environment variables.\n\n", "image": "Screenshot 2024-12-27 at 12.05.57.png"}, {"text": "B47a\n\nQ714 A startup company is hosting a website for its customers on an Amazon EC2 instance.The website consists of a\nstateless Python application and a MySQL database.The website serves only a small amount of traffic.The company is\nconcerned about the reliability of the instance and needs to migrate to a highly available architecture.The company cannot\nmodify the application code.Which combination of actions should a solutions architect take to achieve high availability for\n\nthe website? (Choose two.)\n\nA. Provision an internet gateway in each Availability Zone in use.\n\nB. Migrate the database to an Amazon RDS for MySQL Multi-AZ DB instance.\n\nC. Migrate the database to Amazon DynamoDB, and enable DynamoDB auto scaling.\n\nD. Use AWS DataSync to synchronize the database data across multiple EC2 instances.\n\nE. Create an Application Load Balancer to distribute traffic to an Auto Scaling group of EC2 instances that are\n\ndistributed across two Availability Zones.\n\n", "image": "Screenshot 2024-12-27 at 12.01.23.png"}, {"text": "Ao\n\nQ731 A company is migrating its multi-tier on-premises application to AWS.The application consists of a single-node\nMySQL database and a multi-node web tier.The company must minimize changes to the application during the\nmigration.The company wants to improve application resiliency after the migration.Which combination of steps will meet\nthese requirements? (Choose two.)\n\nA. Migrate the web tier to Amazon EC2 instances in an Auto Scaling group behind an Application Load Balancer.\n\nB. Migrate the database to Amazon EC2 instances in an Auto Scaling group behind a Network Load Balancer.\n\nC. Migrate the database to an Amazon RDS Multi-AZ deployment.\n\nD. Migrate the web tier to an AWS Lambda function.\n\nE. Migrate the database to an Amazon DynamoDB table.\n", "image": "Screenshot 2024-12-27 at 12.01.37.png"}, {"text": "8088\n\nQ844 A company needs a solution to prevent photos with unwanted content from being uploaded to the company's web\napplication. The solution must not involve training a machine learning (ML) model. Which solution will meet these\n\nrequirements?\n\nA. Create and deploy a model by using Amazon SageMaker Autopilot. Create a real-time endpoint that the web\napplication invokes when new photos are uploaded.\n\nB. Create an AWS Lambda function that uses Amazon Rekognition to detect unwanted content. Create a Lambda\n\nfunction URL that the web application invokes when new photos are uploaded.\n\nCc. Create an Amazon CloudFront function that uses Amazon Comprehend to detect unwanted content. Associate the\nfunction with the web application.\n\nD. Create an AWS Lambda function that uses Amazon Rekognition Video to detect unwanted content. Create a Lambda\nfunction URL that the web application invokes when new photos are uploaded.\n", "image": "Screenshot 2024-12-27 at 12.06.03.png"}, {"text": "53136\n\nQ1037 A company wants to use Amazon Elastic Container Service (Amazon ECS) to run its on-premises application in a\nhybrid environment.The application currently runs on containers on premises.The company needs a single container\nsolution that can scale in an on-premises, hybrid, or cloud environment.The company must run new application containers\nin the AWS Cloud and must use a load balancer for HTTP traffic.Which combination of actions will meet these\nrequirements? (Choose two.)\n\nA. Set up an ECS cluster that uses the AWS Fargate launch type for the cloud application containers.Use an Amazon\n\nECS Anywhere external launch type for the on-premises application containers.\n\nB. Set up an Application Load Balancer for cloud ECS services.\n\nc. Set up a Network Load Balancer for cloud ECS services.\n\nD. Set up an ECS cluster that uses the AWS Fargate launch type.Use Fargate for the cloud application containers and\nthe on-premises application containers.\n\nE. Set up an ECS cluster that uses the Amazon EC2 launch type for the cloud application containers.Use Amazon ECS\nAnywhere with an AWS Fargate launch type for the on-premises application containers.\n", "image": "Screenshot 2024-12-27 at 12.11.33.png"}, {"text": "08a\n\nQ5 A company has applications that run in an organization in AWS Organizations.The company outsources operational\nsupport of the applications. The company needs to provide access for the external support engineers without\ncompromising security.The external support engineers need access to the AWS Management Console.The external\nsupport engineers also need operating system access to the company's fleet ofAmazon EC2 instances that run Amazon\nLinux in private subnets.Which solution will meet these requirements MOST securely?\n\nA. Confirm that AWS Systems Manager Agent (SSM Agent) is installed on all instances.Assign an instance profile with\n\nthe necessary policy to connect to Systems Manager.Use AWS IAM Identity Center to provide the external support\nengineers console access.Use Systems Manager Session Manager to assign the required permissions.\n\nB. Confirm that AWS Systems Manager Agent (SSM Agent) is installed on all instances.Assign an instance profile with\nthe necessary policy to connect to Systems Manager.Use Systems Manager Session Manager to provide local IAM\nuser credentials in each AWS account to the external support engineers for console access.\n\nCc. Confirm that all instances have a security group that allows SSH access only from the external support engineers\u2019\nsource IP address ranges.Provide local IAM user credentials in each AWS account to the external support engineers\nfor console access.Provide each external support engineer an SSH key pair to log in to the application instances.\n\nD. Create a bastion host in a public subnet.Set up the bastion host security group to allow access from only the external\nengineers\u2019 IP address ranges.Ensure that all instances have a security group that allows SSH access from the bastion\nhost.Provide each external support engineer an SSH key pair to log in to the application instances.Provide local\naccount IAM user credentials to the engineers for console access.\n", "image": "Screenshot 2024-12-27 at 12.07.43.png"}, {"text": "554m\n\nQ743 A company is building a microservices-based application that will be deployed on Amazon Elastic Kubernetes\nService (Amazon EKS). The microservices will interact with each other. The company wants to ensure that the application\nis observable to identify performance issues in the future. Which solution will meet these requirements?\n\nA. Configure the application to use Amazon ElastiCache to reduce the number of requests that are sent to the\nmicroservices.\n\nB. Configure Amazon CloudWatch Container Insights to collect metrics from the EKS clusters. Configure AWS X-Ray to\n\ntrace the requests between the microservices.\n\nCc. Configure AWS CloudTrail to review the API calls. Build an Amazon QuickSight dashboard to observe the\nmicroservice interactions.\n\nD. Use AWS Trusted Advisor to understand the performance of the application.\n", "image": "Screenshot 2024-12-27 at 12.03.33.png"}, {"text": "8230\n\nQ423 A company hosts a multi-tier web application that uses an Amazon Aurora MySQL DB cluster for storage.The\napplication tier is hosted on Amazon EC2 instances.The company's IT security guidelines mandate that the database\ncredentials be encrypted and rotated every 14 days.What should a solutions architect do to meet this requirement with the\nLEAST operational effort?\n\nA. Create a new AWS Key Management Service (AWS KMS) encryption key.Use AWS Secrets Manager to create a new\n\nsecret that uses the KMS key with the appropriate credentials.Associate the secret with the Aurora DB\ncluster.Configure a custom rotation period of 14 days.\n\nB. Create two parameters in AWS Systems Manager Parameter Store:one for the user name as a string parameter and\none that uses the SecureString type for the password.Select AWS Key Management Service (AWS KMS) encryption\nfor the password parameter, and load these parameters in the application tier.Implement an AWS Lambda function\nthat rotates the password every 14 days.\n\nCc. Store a file that contains the credentials in an AWS Key Management Service (AWS KMS) encrypted Amazon Elastic\nFile System (Amazon EFS) file system.Mount the EFS file system in all EC2 instances of the application tier.Restrict\nthe access to the file on the file system so that the application can read the file and that only super users can modify\nthe file.Implement an AWS Lambda function that rotates the key in Aurora every 14 days and writes new credentials\ninto the file.\n\nD. Store a file that contains the credentials in an AWS Key Management Service (AWS KMS) encrypted Amazon S3\nbucket that the application uses to load the credentials.Download the file to the application regularly to ensure that\nthe correct credentials are used.Implement an AWS Lambda function that rotates the Aurora credentials every 14\ndays and uploads these credentials to the file in the S3 bucket.\n", "image": "Screenshot 2024-12-27 at 11.41.36.png"}, {"text": "S53m\n\nQ741 A company has a web application that includes an embedded NoSQL database. The application runs on Amazon EC2\ninstances behind an Application Load Balancer (ALB). The instances run in an Amazon EC2 Auto Scaling group in a single\nAvailability Zone. A recent increase in traffic requires the application to be highly available and for the database to be\neventually consistent. Which solution will meet these requirements with the LEAST operational overhead?\n\nA. Replace the ALB with a Network Load Balancer. Maintain the embedded NoSQL database with its replication service\non the EC2 instances.\n\nB. Replace the ALB with a Network Load Balancer. Migrate the embedded NoSQL database to Amazon DynamoDB by\nusing AWS Database Migration Service (AWS DMS).\n\nC. Modify the Auto Scaling group to use EC2 instances across three Availability Zones. Maintain the embedded NoSQL\ndatabase with its replication service on the EC2 instances.\n\nD. Modify the Auto Scaling group to use EC2 instances across three Availability Zones. Migrate the embedded NoSQL\n\ndatabase to Amazon DynamoDB by using AWS Database Migration Service (AWS DMS).\n\n", "image": "Screenshot 2024-12-27 at 12.03.27.png"}, {"text": "oom\n\nQ755 A company is using an Application Load Balancer (ALB) to present its application to the internet. The company finds\nabnormal traffic access patterns across the application. A solutions architect needs to improve visibility into the\ninfrastructure to help the company understand these abnormalities better. What is the MOST operationally efficient\n\nsolution that meets these requirements?\n\nA. Create a table in Amazon Athena for AWS CloudTrail logs. Create a query for the relevant information.\n\nB. Enable ALB access logging to Amazon S3. Create a table in Amazon Athena, and query the logs.\n\nCc. Enable ALB access logging to Amazon S3. Open each file in a text editor, and search each line for the relevant\ninformation.\n\nD. Use Amazon EMR on a dedicated Amazon EC2 instance to directly query the ALB to acquire traffic access log\ninformation.\n", "image": "Screenshot 2024-12-27 at 12.04.06.png"}, {"text": "21a\nQ371 As part of budget planning, management wants a report of AWS billed items listed by user.The data will be used to\ncreate department budgets.A solutions architect needs to determine the most efficient way to obtain this report\n\ninformation.Which solution meets these requirements?\n\nA. Run a query with Amazon Athena to generate the report.\n\nB. Create a report in Cost Explorer and download the report.\n\nc. Access the bill details from the billing dashboard and download the bill.\n\nD. Modify a cost budget in AWS Budgets to alert with Amazon Simple Email Service (Amazon SES).\n", "image": "Screenshot 2024-12-27 at 11.41.20.png"}, {"text": "Be\n\nQ155 An application allows users at a company's headquarters to access product data.The product data is stored in an\nAmazon RDS MySQL DB instance.The operations team has isolated an application performance slowdown and wants to\nseparate read traffic from write traffic.A solutions architect needs to optimize the application's performance quickly.What\nshould the solutions architect recommend?\n\nA. Change the existing database to a Multi-AZ deployment.Serve the read requests from the primary Availability Zone.\n\nB. Change the existing database to a Multi-AZ deployment.Serve the read requests from the secondary Availability\nZone.\n\nC. Create read replicas for the database.Configure the read replicas with half of the compute and storage resources as\nthe source database.\n\nD. Create read replicas for the database.Configure the read replicas with the same compute and storage resources as\n\nthe source database.\n\n", "image": "Screenshot 2024-12-27 at 11.36.55.png"}, {"text": "551218\n\nQ28 A company is developing an application in the AWS Cloud.The application's HTTP API contains critical information\nthat is published in Amazon API Gateway.The critical information must be accessible from only a limited set of trusted IP\naddresses that belong to the company's internal network.Which solution will meet these requirements?\n\nA. Set up an API Gateway private integration to restrict access to a predefined set of IP addresses.\n\nB. Create a resource policy for the API that denies access to any IP address that is not specifically allowed.\n\nC. Directly deploy the API in a private subnet.Create a network ACL.Set up rules to allow the traffic from specific IP\naddresses.\n\nD. Modify the security group that is attached to API Gateway to allow inbound traffic from only the trusted IP addresses.\n", "image": "Screenshot 2024-12-27 at 12.09.44.png"}, {"text": "582m\n\nQ884 A company needs to create an AWS Lambda function that will run in a VPC in the company's primary AWS account.\nThe Lambda function needs to access files that the company stores in an Amazon Elastic File System (Amazon EFS) file\nsystem. The EFS file system is located in a secondary AWS account. As the company adds files to the file system, the\nsolution must scale to meet the demand. Which solution will meet these requirements MOST cost-effectively?\n\nA. Create a new EFS file system in the primary account. Use AWS DataSync to copy the contents of the original EFS file\nsystem to the new EFS file system.\n\nB. Create a VPC peering connection between the VPCs that are in the primary account and the secondary account.\n\nCc. Create a second Lambda function in the secondary account that has a mount that is configured for the file system.\nUse the primary account's Lambda function to invoke the secondary account's Lambda function.\n\nD. Move the contents of the file system to a Lambda layer. Configure the Lambda layer's permissions to allow the\ncompany's secondary account to use the Lambda layer.\n", "image": "Screenshot 2024-12-27 at 12.06.15.png"}, {"text": "151\n\nQ938 A company is running a media store across multiple Amazon EC2 instances distributed across multiple Availability\nZones in a single VPC.The company wants a high-performing solution to share data between all the EC2 instances, and\nprefers to keep the data within the VPC only.What should a solutions architect recommend?\n\nA. Create an Amazon S3 bucket and call the service APIs from each instance's application\nB. Create an Amazon S3 bucket and Configure all instances to access it as a mounted volume\nc. Configure an Amazon Elastic Block Store (Amazon EBS) volume and mount it across all instances\n\nD. Configure an Amazon Elastic File System (Amazon EFS) file system and mount it across all instances\n", "image": "Screenshot 2024-12-27 at 12.12.48.png"}, {"text": "S36m\n\nQ542 A company has an application that processes customer orders.The company hosts the application on an Amazon\nEC2 instance that saves the orders to an Amazon Aurora database.Occasionally when traffic is high the workload does not\nprocess orders fast enough.What should a solutions architect do to write the orders reliably to the database as quickly as\npossible?\n\nA. Increase the instance size of the EC2 instance when traffic is high.Write orders to Amazon Simple Notification\nService (Amazon SNS).Subscribe the database endpoint to the SNS topic.\n\nB. Write orders to an Amazon Simple Queue Service (Amazon SQS) queue.Use EC2 instances in an Auto Scaling group\n\nbehind an Application Load Balancer to read from the SQS queue and process orders into the database.\n\nCc. Write orders to Amazon Simple Notification Service (Amazon SNS).Subscribe the database endpoint to the SNS\ntopic.Use EC2 instances in an Auto Scaling group behind an Application Load Balancer to read from the SNS topic.\n\nD. Write orders to an Amazon Simple Queue Service (Amazon SQS) queue when the EC2 instance reaches CPU\nthreshold limits.Use scheduled scaling of EC2 instances in an Auto Scaling group behind an Application Load\nBalancer to read from the SQS queue and process orders into the database.\n", "image": "Screenshot 2024-12-27 at 11.57.44.png"}, {"text": "5510288\n\nQQ A company has a multi-tier web application.The application's internal service components are deployed on Amazon Ec2\ninstances.Theinternal service components need to access third-party software as a service (SaaS) APls that are hosted on\nAWS.The company needs to provide secure and private connectivity from the application's internal services to the third-\n\nparty SaaS application.thecompany needs to ensure that there is minimal public internet exposure.Which solution will meet\n\nthese requirements?\n\nA. Implement an AWS Site-to-Site VPN to establish a secure connection with the third-party SaaS provider\n\nB. Deploy AWs Transit Gateway to manage and route trafic between the application's VPc and the third-party SaaS\nprovider.\n\nCc. Confgure AWS Privatelink to allow only outbound traic from the VPc without enabling the third-party SaaS provider to\nestablish\n\nD. Use AWS PrivateLink to create a private connection between the application's VPC and the third-party SaaS\n\nprovider.\n\n", "image": "Screenshot 2024-12-27 at 12.08.04.png"}, {"text": "8770\n\nQ829 A company wants to run its experimental workloads in the AWS Cloud. The company has a budget for cloud\nspending. The company's CFO is concerned about cloud spending accountability for each department. The CFO wants to\nreceive notification when the spending threshold reaches 60% of the budget. Which solution will meet these\nrequirements?\n\nA. Use cost allocation tags on AWS resources to label owners. Create usage budgets in AWS Budgets. Add an alert\n\nthreshold to receive notification when spending exceeds 60% of the budget.\n\nB. Use AWS Cost Explorer forecasts to determine resource owners. Use AWS Cost Anomaly Detection to create alert\nthreshold notifications when spending exceeds 60% of the budget.\n\nc. Use cost allocation tags on AWS resources to label owners. Use AWS Support API on AWS Trusted Advisor to create\nalert threshold notifications when spending exceeds 60% of the budget.\n\nD. Use AWS Cost Explorer forecasts to determine resource owners. Create usage budgets in AWS Budgets. Add an\nalert threshold to receive notification when spending exceeds 60% of the budget.\n", "image": "Screenshot 2024-12-27 at 12.05.45.png"}, {"text": "378ml\n\nQ838 A company's web application that is hosted in the AWS Cloud recently increased in popularity. The web application\ncurrently exists on a single Amazon EC2 instance in a single public subnet. The web application has not been able to meet\nthe demand of the increased web traffic. The company needs a solution that will provide high availability and scalability to\nmeet the increased user demand without rewriting the web application. Which combination of steps will meet these\nrequirements? (Choose two.)\n\nA. Replace the EC2 instance with a larger compute optimized instance.\n\nB. Configure Amazon EC2 Auto Scaling with multiple Availability Zones in private subnets.\n\nCc. Configure a NAT gateway in a public subnet to handle web requests.\n\nD. Replace the EC2 instance with a larger memory optimized instance.\n\nE. Configure an Application Load Balancer in a public subnet to distribute web traffic.\n", "image": "Screenshot 2024-12-27 at 12.05.51.png"}, {"text": "#1338\n\nQ1063 A company runs its customer-facing web application on containers.The workload uses Amazon Elastic Container\nService (Amazon ECS) on AWS Fargate.The web application is resource intensive.The web application needs to be\navailable 24 hours a day, 7 days a week for customers.The company expects the application to experience short bursts of\nhigh traffic. The workload must be highly available.Which solution will meet these requirements MOST cost-effectively?\n\nA. Configure an ECS capacity provider with Fargate.Conduct load testing by using a third-party tool.Rightsize the\nFargate tasks in Amazon CloudWatch.\n\nB. Configure an ECS capacity provider with Fargate for steady state and Fargate Spot for burst traffic.\n\nCc. Configure an ECS capacity provider with Fargate Spot for steady state and Fargate for burst traffic.\n\nD. Configure an ECS capacity provider with Fargate.Use AWS Compute Optimizer to rightsize the Fargate task.\n", "image": "Screenshot 2024-12-27 at 12.11.18.png"}, {"text": "551298\n\nQ36 A company currently stores 5 TB of data in on-premises block storage systems.The company's current storage\nsolution provides limited space for additional data. The company runs applications on premises that must be able to retrieve\nfrequently accessed data with low latency.The company requires a cloud-based storage solution.Which solution will meet\nthese requirements with the MOST operational efficiency?\n\nA. Use Amazon S3 File Gateway.Integrate S3 File Gateway with the on-premises applications to store and directly\nretrieve files by using the SMB file system.\n\nB. Use an AWS Storage Gateway Volume Gateway with cached volumes as iSCSI targets.\nc. Use an AWS Storage Gateway Volume Gateway with stored volumes as iSCSI targets.\n\nD. Use an AWS Storage Gateway Tape Gateway.Integrate Tape Gateway with the on-premises applications to store\nvirtual tapes in Amazon S3.\n", "image": "Screenshot 2024-12-27 at 12.10.58.png"}, {"text": "61g\nQ756 A company wants to use NAT gateways in its AWS environment. The company's Amazon EC2 instances in private\n\nsubnets must be able to connect to the public internet through the NAT gateways. Which solution will meet these\nrequirements?\n\nA. Create public NAT gateways in the same private subnets as the EC2 instances.\n\nB. Create private NAT gateways in the same private subnets as the EC2 instances.\n\nC. Create public NAT gateways in public subnets in the same VPCs as the EC2 instances.\n\nD. Create private NAT gateways in public subnets in the same VPCs as the EC2 instances.\n", "image": "Screenshot 2024-12-27 at 12.04.11.png"}, {"text": "aoa\n\nQ49 A company has implemented a self-managed DNS solution on three Amazon EC2 instances behind a Network Load\nBalancer (NLB) in the us-west-2 Region.Most of the company's users are located in the United States and Europe.The\ncompany wants to improve the performance and availability of the solution. The company launches and configures three\nEC2 instances in the eu-west-1 Region and adds the EC2 instances as targets for anew NLBWhich solution can the\ncompany use to route traffic to all the EC2 instances?\n\nA. Create an Amazon Route 53 geolocation routing policy to route requests to one of the two NLBs.Create an Amazon\nCloudFront distribution.Use the Route 53 record as the distribution's origin.\n\nB. Create a standard accelerator in AWS Global Accelerator.Create endpoint groups in us-west-2 and eu-west-1.Add\n\nthe two NLBs as endpoints for the endpoint groups.\n\nCc. Attach Elastic IP addresses to the six EC2 instances.Create an Amazon Route 53 geolocation routing policy to route\nrequests to one of the six EC2 instances.Create an Amazon CloudFront distribution.Use the Route 53 record as the\ndistribution's origin.\n\nD. Replace the two NLBs with two Application Load Balancers (ALBs).Create an Amazon Route 53 latency routing policy\nto route requests to one of the two ALBs.Create an Amazon CloudFront distribution.Use the Route 53 record as the\ndistribution's origin.\n", "image": "Screenshot 2024-12-27 at 11.35.39.png"}, {"text": "Boom\n\nQ2 A company has migrated several applications to AwS in the past 3 months.The company wants to know the breakdown\nof costs for each ofthese applications. The company wants to receive a regular report that includes this information.Which\nsolution will meet these requirements MOST cost-effectively?\n\nA. Use AWS Budgets to download data for the past 3 months into a csyle.Look up the desired information.\n\nB. Load AWS Cost and Usage Reports into an Amazon RDS DB instance.Run SOL queries to get the desired information\n\nC. Tag allthe AWS resources with a key for cost and a value of the applicaion's name.Activate cost allocation tags.Use\n\nCost Explgre get the desired information.\n\nD. Tag allthe Aws resources with a key for cost and a value of the application's name.Use the Aws Bliling and cost\nManagement consoletadewnlgad bills for the past 3 months.Look up the desired information.\n", "image": "Screenshot 2024-12-27 at 12.07.27.png"}, {"text": "5141\n\nQ1029 A company is migrating a data center from its on-premises location to AWS.The company has several legacy\napplications that are hosted on individual virtual servers.Changes to the application designs cannot be made.Each\nindividual virtual server currently runs as its own EC2 instance.A solutions architect needs to ensure that the applications\nare reliable and fault tolerant after migration to AWS.The applications will run on Amazon EC2 instances.Which solution will\nmeet these requirements?\n\nA. Create an Auto Scaling group that has a minimum of one and a maximum of one.Create an Amazon Machine Image\n\n(AMI) of each application instance.Use the AMI to create EC2 instances in the Auto Scaling group Configure an\nApplication Load Balancer in front of the Auto Scaling group.\n\nB. Use AWS Backup to create an hourly backup of the EC2 instance that hosts each application.Store the backup in\nAmazon S3 in a separate Availability Zone.Configure a disaster recovery process to restore the EC2 instance for each\napplication from its most recent backup.\n\nCc. Create an Amazon Machine Image (AMI) of each application instance.Launch two new EC2 instances from the\nAMI.Place each EC2 instance in a separate Availability Zone.Configure a Network Load Balancer that has the EC2\ninstances as targets.\n\nD. Use AWS Mitigation Hub Refactor Spaces to migrate each application off the EC2 instance.Break down functionality\nfrom each application into individual components.Host each application on Amazon Elastic Container Service\n(Amazon ECS) with an AWS Fargate launch type.\n", "image": "Screenshot 2024-12-27 at 12.11.57.png"}, {"text": "1448\nQ1023 A company uses a Microsoft SQL Server database.The company's applications are connected to the database.The\ncompany wants to migrate to an Amazon Aurora PostgreSQL database with minimal changes to the application code.Which\n\ncombination of steps will meet these requirements? (Choose two.)\n\nA. Use the AWS Schema Conversion Tool (AWS SCT) to rewrite the SQL queries in the applications.\n\nB. Enable Babelfish on Aurora PostgreSQL to run the SQL queries from the applications.\n\nC. Migrate the database schema and data by using the AWS Schema Conversion Tool (AWS SCT) and AWS Database\n\nMigration Service (AWS DMS).\n\nD. Use Amazon RDS Proxy to connect the applications to Aurora PostgreSQL.\n\nE. Use AWS Database Migration Service (AWS DMS) to rewrite the SQL queries in the applications.\n", "image": "Screenshot 2024-12-27 at 12.12.12.png"}, {"text": "1430\n\nQ1024 A company plans to rehost an application to Amazon EC2 instances that use Amazon Elastic Block Store (Amazon\nEBS) as the attached storage.A solutions architect must design a solution to ensure that all newly created Amazon EBS\nvolumes are encrypted by default.The solution must also prevent the creation of unencrypted EBS volumes.Which solution\n\nwill meet these requirements?\n\nA. Configure the EC2 account attributes to always encrypt new EBS volumes.\n\nB. Use AWS Config.Configure the encrypted-volumes identifier.Apply the default AWS Key Management Service (AWS\nKMS) key.\n\nc. Configure AWS Systems Manager to create encrypted copies of the EBS volumes.ReConfigure the EC2 instances to\nuse the encrypted volumes.\n\nD. Create a customer managed key in AWS Key Management Service (AWS KMS).Configure AWS Migration Hub to use\nthe key when the company migrates workloads.\n", "image": "Screenshot 2024-12-27 at 12.12.07.png"}, {"text": "534m\n\nQ533 A company wants to implement a backup strategy for Amazon EC2 data and multiple Amazon S3 buckets.Because\nof regulatory requirements, the company must retain backup files for a specific time period. The company must not alter\nthe files for the duration of the retention period.Which solution will meet these requirements?\n\nA. Use AWS Backup to create a backup vault that has a vault lock in governance mode.Create the required backup plan.\nB. Use Amazon Data Lifecycle Manager to create the required automated snapshot policy.\n\nc. Use Amazon S3 File Gateway to create the backup.Configure the appropriate S3 Lifecycle management.\n\nD. Use AWS Backup to create a backup vault that has a vault lock in compliance mode.Create the required backup plan.\n", "image": "Screenshot 2024-12-27 at 11.57.23.png"}, {"text": "#1388\n\nQ1035 A company runs workloads in the AWS Cloud.The company wants to centrally collect security data to assess\nsecurity across the entire company and to improve workload protection.Which solution will meet these requirements with\n\nthe LEAST development effort?\n\nA. Configure a data lake in AWS Lake Formation.Use AWS Glue crawlers to ingest the security data into the data lake.\n\nB. Configure an AWS Lambda function to collect the security data in.csv format.Upload the data to an Amazon S3\nbucket.\n\nCc. Configure a data lake in Amazon Security Lake to collect the security data.Upload the data to an Amazon S3 bucket.\n\nD. Configure an AWS Database Migration Service (AWS DMS) replication instance to load the security data into an\nAmazon RDS cluster.\n", "image": "Screenshot 2024-12-27 at 12.11.42.png"}, {"text": "52682\n\nQ468 A solutions architect is creating a new VPC design.There are two public subnets for the load balancer, two private\nsubnets for web servers and two private subnets for MySQL.The web servers use only HTTPS.The solutions architect has\nalready created a security group tor the load balancer allowing port 443 from 0.0.0.0/0.Company policy requires that each\nresource has the teas! access required to still be able to perform its tasks.Which additional configuration strategy should\n\nthe solutions architect use to meet these requirements?\n\nA. Create a security group for the web servers and allow port 443 from 0.0.0.0/0.Create a security group for the MySQL\nservers and allow port 3306 from the web servers security group.\n\nB. Create a network ACL for the web servers and allow port 443 from 0.0.0.0/0.Create a network ACL for the MySQL\nservers and allow port 3306 from the web servers security group.\n\nC. Create a security group for the web servers and allow port 443 from the load balancer.Create a security group for the\n\nMySQL servers and allow port 3306 from the web servers security group.\n\nD. Create a network ACL for the web servers and allow port 443 from the load balancer.Create a network ACL for the\nMySQL servers and allow port 3306 from the web servers security group.\n", "image": "Screenshot 2024-12-27 at 11.42.02.png"}, {"text": "1178\n\nQ24 A company's software development team needs an Amazon RDS Multi-AZ cluster.The RDS cluster will serve as a\nbackend for a desktop client that is deployed on premises.The desktop client requires direct connectivity to the RDS\ncluster.The company must give the development team the ability to connect to the cluster by using the client when the\nteam is in the office.Which solution provides the required connectivity MOST securely?\n\nA. Create a VPC and two public subnets.Create the RDS cluster in the public subnets.Use AWS Site-to-Site VPN with a\ncustomer gateway in the company's office.\n\nB. Create a VPC and two private subnets.Create the RDS cluster in the private subnets.Use AWS Site-to-Site VPN with a\ncustomer gateway in the company's office.\n\nCc. Create a VPC and two private subnets.Create the RDS cluster in the private subnets.Use RDS security groups to\n\nallow the company's office IP ranges to access the cluster.\n\nD. Create a VPC and two public subnets.Create the RDS cluster in the public subnets.Create a cluster user for each\ndeveloper.Use RDS security groups to allow the users to access the cluster.\n", "image": "Screenshot 2024-12-27 at 12.09.23.png"}, {"text": "OG Ri\n\nQ3 A company has two VPCs that are located in the us-west-2 Region within the same AWS account.The company needs\nto allow network traffic between these VPCs.Approximately 500 GB of data transfer will occur between the VPCs each\nmonth.What is the MOST cost-effective solution to connect these VPCs?\n\nA. Implement AWS Transit Gateway to connect the VPCs.Update the route tables of each VPC to use the transit\ngateway for inter-VPC communication.\n\nB. Implement an AWS Site-to-Site VPN tunnel between the VPCs.Update the route tables of each VPC to use the VPN\ntunnel for inter-VPC communication.\n\nc. Set up a VPC peering connection between the VPCs.Update the route tables of each VPC to use the VPC peering\n\nconnection for inter-VPC communication.\n\nD. Set up a 1 GB AWS Direct Connect connection between the VPCs.Update the route tables of each VPC to use the\nDirect Connect connection for inter-VPC communication.\n", "image": "Screenshot 2024-12-27 at 12.07.32.png"}, {"text": "25g\n\nQ437 A company uses a 100 GB Amazon RDS for Microsoft SQL Server Single-AZ DB instance in the us-east-1 Region to\nstore customer transactions.The company needs high availability and automatic recovery for the DB instance.The company\nmust also run reports on the RDS database several times a year.The report process causes transactions to take longer than\nusual to post to the customers' accounts.The company needs a solution that will improve the performance of the report\nprocess.Which combination of steps will meet these requirements? (Choose two.)\n\nA. Modify the DB instance from a Single-AZ DB instance to a Multi-AZ deployment.\n\nB. Take a snapshot of the current DB instance.Restore the snapshot to a new RDS deployment in another Availability\nZone.\n\nCc. Create a read replica of the DB instance in a different Availability Zone.Point all requests for reports to the read\n\nreplica.\n\nD. Migrate the database to RDS Custom.\n\nE. Use RDS Proxy to limit reporting requests to the maintenance window.\n", "image": "Screenshot 2024-12-27 at 11.41.53.png"}, {"text": "58754\n\nQ822 A solutions architect must provide an automated solution for a company's compliance policy that states security\ngroups cannot include a rule that allows SSH from 0.0.0.0/0. The company needs to be notified if there is any breach in the\npolicy. A solution is needed as soon as possible. What should the solutions architect do to meet these requirements with\n\nthe LEAST operational overhead?\n\nA. Write an AWS Lambda script that monitors security groups for SSH being open to 0.0.0.0/0 addresses and creates a\nnotification every time it finds one.\n\nB. Enable the restricted-ssh AWS Config managed rule and generate an Amazon Simple Notification Service (Amazon\n\nSNS) notification when a noncompliant rule is created.\n\nC. Create an IAM role with permissions to globally open security groups and network ACLs. Create an Amazon Simple\nNotification Service (Amazon SNS) topic to generate a notification every time the role is assumed by a user.\n\nD. Configure a service control policy (SCP) that prevents non-administrative users from creating or editing security\ngroups. Create a notification in the ticketing system when a user requests a rule that needs administrator\n\npermissions.\n", "image": "Screenshot 2024-12-27 at 12.05.35.png"}, {"text": "oom\n\nQ900 A company has an on-premises application that uses SFTP to collect financial data from multiple vendors. The\ncompany is migrating to the AWS Cloud. The company has created an application that uses Amazon S3 APIs to upload\nfiles from vendors. Some vendors run their systems on legacy applications that do not support S3 APIs. The vendors want\nto continue to use SFTP-based applications to upload data. The company wants to use managed services for the needs of\nthe vendors that use legacy applications. Which solution will meet these requirements with the LEAST operational\n\noverhead?\n\nA. Create an AWS Database Migration Service (AWS DMS) instance to replicate data from the storage of the vendors\nthat use legacy applications to Amazon S3. Provide the vendors with the credentials to access the AWS DMS\ninstance.\n\nB. Create an AWS Transfer Family endpoint for vendors that use legacy applications.\n\nc. Configure an Amazon EC2 instance to run an SFTP server. Instruct the vendors that use legacy applications to use\nthe SFTP server to upload data.\n\nD. Configure an Amazon S3 File Gateway for vendors that use legacy applications to upload files to an SMB file share.\n", "image": "Screenshot 2024-12-27 at 12.06.59.png"}, {"text": "5518\nQ737 A company wants to standardize its Amazon Elastic Block Store (Amazon EBS) volume encryption strategy. The\ncompany also wants to minimize the cost and configuration effort required to operate the volume encryption check. Which\n\nsolution will meet these requirements?\n\nA. Write API calls to describe the EBS volumes and to confirm the EBS volumes are encrypted. Use Amazon\nEventBridge to schedule an AWS Lambda function to run the API calls.\n\nB. Write API calls to describe the EBS volumes and to confirm the EBS volumes are encrypted. Run the API calls on an\nAWS Fargate task.\n\nCc. Create an AWS Identity and Access Management (IAM) policy that requires the use of tags on EBS volumes. Use\nAWS Cost Explorer to display resources that are not properly tagged. Encrypt the untagged resources manually.\n\nD. Create an AWS Config rule for Amazon EBS to evaluate if a volume is encrypted and to flag the volume if it is not\n\nencrypted.\n\n", "image": "Screenshot 2024-12-27 at 12.01.50.png"}, {"text": "50m\n\nQ733 A company's ecommerce website has unpredictable traffic and uses AWS Lambda functions to directly access a\nprivate Amazon RDS for PostgreSQL DB instance. The company wants to maintain predictable database performance and\nensure that the Lambda invocations do not overload the database with too many connections. What should a solutions\n\narchitect do to meet these requirements?\n\nA. Point the client driver at an RDS custom endpoint. Deploy the Lambda functions inside a VPC.\n\nB. Point the client driver at an RDS proxy endpoint. Deploy the Lambda functions inside a VPC.\n\nCc. Point the client driver at an RDS custom endpoint. Deploy the Lambda functions outside a VPC.\n\nD. Point the client driver at an RDS proxy endpoint. Deploy the Lambda functions outside a VPC.\n", "image": "Screenshot 2024-12-27 at 12.01.44.png"}, {"text": "320\nQ518 A company wants to share accounting data with an external auditor.The data is stored in an Amazon RDS DB\n\ninstance that resides in a private subnet.The auditor has its own AWS account and requires its own copy of the\ndatabase.What is the MOST secure way for the company to share the database with the auditor?\n\nA. Create a read replica of the database.Configure IAM standard database authentication to grant the auditor access.\n\nB. Export the database contents to text files.Store the files in an Amazon S3 bucket.Create a new IAM user for the\nauditor.Grant the user access to the S3 bucket.\n\nCc. Copy a snapshot of the database to an Amazon S3 bucket.Create an IAM user.Share the user's keys with the auditor\nto grant access to the object in the S3 bucket.\n\nD. Create an encrypted snapshot of the database.Share the snapshot with the auditor.Allow access to the AWS Key\n\nManagement Service (AWS KMS) encryption key.\n\n", "image": "Screenshot 2024-12-27 at 11.49.20.png"}, {"text": "S198\n\nQ26 A streaming media company is rebuilding its infrastructure to accommodate increasing demand for video content that\nusers consume daily.The company needs to process terabyte-sized videos to block some content in the videos.Video\nprocessing can take up to 20 minutes.The company needs a solution that will scale with demand and remain cost-\neffective.Which solution will meet these requirements?\n\nA. Use AWS Lambda functions to process videos.Store video metadata in Amazon DynamoDB.Store video content in\nAmazon S3 IntelligentTiering.\n\nB. Use Amazon Elastic Container Service (Amazon ECS) and AWS Fargate to implement microservices to process\n\nvideos.Store video metadata in Amazon Aurora.Store video content in Amazon S3 Intelligent-Tiering.\n\nc. Use Amazon EC2 instances in an Auto Scaling group behind an Application Load Balancer (ALB) to process\nvideos.Store video content in Amazon S3 Standard.Use Amazon Simple Queue Service (Amazon SQS) for queuing\nand to decouple processing tasks.\n\nD. Deploy a containerized video processing application on Amazon Elastic Kubernetes Service (Amazon EKS) on\nAmazon EC2.Store video metadata in Amazon RDS in a single Availability Zone.Store video content in Amazon S3\nGlacier Deep Archive.\n", "image": "Screenshot 2024-12-27 at 12.09.34.png"}, {"text": "a7a\n\nQ115 A solutions architect is designing a shared storage solution for a web application that is deployed across multiple\nAvailability Zones.The web application runs on Amazon EC2 instances that are in an Auto Scaling group.The company plans\nto make frequent changes to the content.The solution must have strong consistency in returning the new content as soon\nas the changes occurWhich solutions meet these requirements? (Select TWO.)\n\nA. Use AWS Storage Gateway Volume Gateway Internet Small Computer Systems Interface (ISCSI) block storage that is\nmounted to the individual EC2 instances.\n\nB. Create an Amazon Elastic File System (Amazon EFS) file system.Mount the EFS file system on the individual EC2\n\ninstances.\n\nC. Create a shared Amazon Elastic Block Store (Amazon EBS) volume.Mount the EBS volume on the individual EC2\ninstances.\n\nD. Use AWS DataSync to perform continuous synchronization of data between EC2 hosts in the Auto Scaling group.\n\nE. Create an Amazon S3 bucket to store the web content.Set the metadata for the Cache-Control header to no-\n\ncache.Use Amazon CloudFront to deliver the content.\n\n", "image": "Screenshot 2024-12-27 at 11.36.19.png"}, {"text": "58m\n\nQ753 A company runs a three-tier application in a VPC. The database tier uses an Amazon RDS for MySQL DB instance.\nThe company plans to migrate the RDS for MySQL DB instance to an Amazon Aurora PostgreSQL DB cluster. The company\nneeds a solution that replicates the data changes that happen during the migration to the new database. Which\ncombination of steps will meet these requirements? (Choose two.)\n\nA. Use AWS Database Migration Service (AWS DMS) Schema Conversion to transform the database objects.\n\nB. Use AWS Database Migration Service (AWS DMS) Schema Conversion to create an Aurora PostgreSQL read replica\non the RDS for MySQL DB instance.\n\nCc. Configure an Aurora MySQL read replica for the RDS for MySQL DB instance.\nD. Define an AWS Database Migration Service (AWS DMS) task with change data capture (CDC) to migrate the data.\n\nE. Promote the Aurora PostgreSQL read replica to a standalone Aurora PostgreSQL DB cluster when the replica lag is\nzero.\n", "image": "Screenshot 2024-12-27 at 12.03.55.png"}, {"text": "824g\n\nQ424 A company has deployed a web application on AWS.The company hosts the backend database on Amazon RDS for\nMySQL with a primary DB instance and five read replicas to support scaling needs.The read replicas must lag no more than\n1 second behind the primary DB instance.The database routinely runs scheduled stored procedures.As traffic on the\nwebsite increases, the replicas experience additional lag during periods of peak load.A solutions architect must reduce the\nreplication lag as much as possible.The solutions architect must minimize changes to the application code and must\nminimize ongoing operational overhead.Which solution will meet these requirements?\n\nA. Migrate the database to Amazon Aurora MySQL.Replace the read replicas with Aurora Replicas, and configure Aurora\n\nAuto Scaling.Replace the stored procedures with Aurora MySQL native functions.\n\nB. Deploy an Amazon ElastiCache for Redis cluster in front of the database.Modify the application to check the cache\nbefore the application queries the database.Replace the stored procedures with AWS Lambda functions.\n\nC. Migrate the database to a MySQL database that runs on Amazon EC2 instances.Choose large, compute optimized\nEC2 instances for all replica nodes.Maintain the stored procedures on the EC2 instances.\n\nD. Migrate the database to Amazon DynamoDB.Provision a large number of read capacity units (RCUs) to support the\nrequired throughput, and configure on-demand capacity scaling.Replace the stored procedures with DynamoDB\nstreams.\n", "image": "Screenshot 2024-12-27 at 11.41.44.png"}, {"text": "56m\n\nQ747 A company has an application that uses Docker containers in its local data center. The application runs on a\ncontainer host that stores persistent data in a volume on the host. The container instances use the stored persistent data.\nThe company wants to move the application to a fully managed service because the company does not want to manage\nany servers or storage infrastructure. Which solution will meet these requirements?\n\nA. Use Amazon Elastic Kubernetes Service (Amazon EKS) with self-managed nodes. Create an Amazon Elastic Block\n\nStore (Amazon EBS) volume attached to an Amazon EC2 instance. Use the EBS volume as a persistent volume\nmounted in the containers.\n\nB. Use Amazon Elastic Container Service (Amazon ECS) with.an AWS Fargate launch type. Create an Amazon Elastic\n\nFile System (Amazon EFS) volume. Add the EFS volume as a persistent storage volume mounted in the containers.\n\nc. Use Amazon Elastic Container Service (Amazon ECS) with an AWS Fargate launch type. Create an Amazon S3\nbucket. Map the S3 bucket as a persistent storage volume mounted in the containers.\n\nD. Use Amazon Elastic Container Service (Amazon ECS) with an Amazon EC2 launch type. Create an Amazon Elastic\nFile System (Amazon EFS) volume. Add the EFS volume as a persistent storage volume mounted in the containers.\n", "image": "Screenshot 2024-12-27 at 12.03.45.png"}, {"text": "6Om\n\nQ802 A company has users all around the world accessing its HTTP-based application deployed on Amazon EC2\ninstances in multiple AWS Regions. The company wants to improve the availability and performance of the application. The\ncompany also wants to protect the application against common web exploits that may affect availability, compromise\nsecurity, or consume excessive resources. Static IP addresses are required. What should a solutions architect recommend\nto accomplish this?\n\nA. Put the EC2 instances behind Network Load Balancers (NLBs) in each Region. Deploy AWS WAF on the NLBs. Create\nan accelerator using AWS Global Accelerator and register the NLBs as endpoints.\n\nB. Put the EC2 instances behind Application Load Balancers (ALBs) in each Region. Deploy AWS WAF on the ALBs.\n\nCreate an accelerator using AWS Global Accelerator and register the ALBs as endpoints.\n\nc. Put the EC2 instances behind Network Load Balancers (NLBs) in each Region. Deploy AWS WAF on the NLBs. Create\nan Amazon CloudFront distribution with an origin that uses Amazon Route 53 latency-based routing to route\nrequests to the NLBs.\n\nD. Put the EC2 instances behind Application Load Balancers (ALBs) in each Region. Create an Amazon CloudFront\ndistribution with an origin that uses Amazon Route 53 latency-based routing to route requests to the ALBs. Deploy\nAWS WAF on the CloudFront distribution.\n", "image": "Screenshot 2024-12-27 at 12.04.58.png"}, {"text": "OAR\n\nQ1 Topic 1 An application running on an Amazon EC2 instance in VPC-A needs to access les in another EC2 instance in\nVPC-B.Both VPCs are in separate AWS accounts.The network administrator needs to design a solution to congure secure\naccess to EC2 instance in VPC-B from VPCA.The connectivity should not have a single point of failure or bandwidth\nconcerns.Which solution will meet these requirements?\n\nA. Set up a VPC peering connection between VPC-A and VPC-B.\n\nB. Set up VPC gateway endpoints for the EC2 instance running in VPC-B.\nCc. Attach a virtual private gateway to VPC-B and set up routing from VPC-A.\n\nD. Create a private virtual interface (VIF) for the EC2 instance running in VPC-B and add appropriate routes from VPC-\nA.\n", "image": "Screenshot 2024-12-27 at 12.07.21.png"}, {"text": "S168\n\nQ23 A company hosts its core network services, including directory services and DNS.in its on-premises data center.The\ndata center is connected to the AWS Cloud using AWS Direct Connect (DX) Additional AWS accounts are planned that will\nrequire quick, cost-effective, and consistent access to these network services What should a solutions architect\nimplement to meet these requirements with the LEAST amount of operational overhead?\n\nA. Create a DX connection in each new account.Route the network traffic to the on-premises servers\nB. Configure VPC endpoints in the DX VPC for all required services.Route the network traffic to the on-premises servers\n\nCc. Create a VPN connection between each new account and the DX VPp Route the network traffic to the on-premises\nservers\n\nD. Configure AWS Transit Gateway between the accounts.Assign DX to the transit gateway and route network traffic to\n\nthe on-premises servers\n\n", "image": "Screenshot 2024-12-27 at 12.09.18.png"}, {"text": "8720\n\nQ818 A company's application is deployed on Amazon EC2 instances and uses AWS Lambda functions for an event-driven\narchitecture. The company uses nonproduction development environments in a different AWS account to test new features\nbefore the company deploys the features to production. The production instances show constant usage because of\ncustomers in different time zones. The company uses nonproduction instances only during business hours on weekdays.\nThe company does not use the nonproduction instances on the weekends. The company wants to optimize the costs to\nrun its application on AWS. Which solution will meet these requirements MOST cost-effectively?\n\nA. Use On-Demand Instances for the production instances. Use Dedicated Hosts for the nonproduction instances on\nweekends only.\n\nB. Use Reserved Instances for the production instances and the nonproduction instances. Shut down the\nnonproduction instances when not in use.\n\nc. Use Compute Savings Plans for the production instances. Use On-Demand Instances for the nonproduction\n\ninstances. Shut down the nonproduction instances when not in use.\n\nD. Use Dedicated Hosts for the production instances. Use EC2 Instance Savings Plans for the nonproduction instances.\n", "image": "Screenshot 2024-12-27 at 12.05.18.png"}, {"text": "5B 73K\n\nQ819 A company stores data in an on-premises Oracle relational database. The company needs to make the data available\nin Amazon Aurora PostgreSQL for analysis. The company uses an AWS Site-to-Site VPN connection to connect its on-\npremises network to AWS. The company must capture the changes that occur to the source database during the migration\nto Aurora PostgreSQL. Which solution will meet these requirements?\n\nA. Use the AWS Schema Conversion Tool (AWS SCT) to convert the Oracle schema to Aurora PostgreSQL schema. Use\nthe AWS Database Migration Service (AWS DMS) full-load migration task to migrate the data.\n\nB. Use AWS DataSync to migrate the data to an Amazon S3 bucket. Import the S3 data to Aurora PostgreSQL by using\nthe Aurora PostgreSQL aws_s3 extension.\n\nc. Use the AWS Schema Conversion Tool (AWS SCT) to convert the Oracle schema to Aurora PostgreSQL schema. Use\n\nAWS Database Migration Service (AWS DMS) to migrate the existing data and replicate the ongoing changes.\n\nD. Use an AWS Snowball device to migrate the data to an Amazon S3 bucket. Import the S3 data to Aurora PostgreSQL\nby using the Aurora PostgreSQL aws_s3 extension.\n", "image": "Screenshot 2024-12-27 at 12.05.24.png"}, {"text": "#14088\n\nQ1033 A company is designing its production application's disaster recovery (DR) strategy.The application is backed by a\nMySQL database on an Amazon Aurora cluster in the us-east-1 Region.The company has chosen the us-west-1 Region as\nits DR Region.The company's target recovery point objective (RPO) is 5 minutes and the target recovery time objective\n(RTO) is 20 minutes.The company wants to minimize Configuration changes.Which solution will meet these requirements\nwith the MOST operational efficiency?\n\nA. Create an Aurora read replica in us-west-1 similar in size to the production application's Aurora MySQL cluster writer\ninstance.\n\nB. Convert the Aurora cluster to an Aurora global database.Configure managed failover.\n\nC. Create a new Aurora cluster in us-west-1 that has Cross-Region Replication.\n\nD. Create a new Aurora cluster in us-west-1.Use AWS Database Migration Service (AWS DMS) to sync both clusters.\n", "image": "Screenshot 2024-12-27 at 12.11.51.png"}, {"text": "5574\n\nQ821 An ecommerce company is running a seasonal online sale. The company hosts its website on Amazon EC2 instances\nspanning multiple Availability Zones. The company wants its website to manage sudden traffic increases during the sale.\nWhich solution will meet these requirements MOST cost-effectively?\n\nA. Create an Auto Scaling group that is large enough to handle peak traffic load. Stop half of the Amazon EC2\ninstances. Configure the Auto Scaling group to use the stopped instances to scale out when traffic increases.\n\nB. Create an Auto Scaling group for the website. Set the minimum size of the Auto Scaling group so that it can handle\nhigh traffic volumes without the need to scale out.\n\nc. Use Amazon CloudFront and Amazon ElastiCache to cache dynamic content with an Auto Scaling group set as the\norigin. Configure the Auto Scaling group with the instances necessary to populate CloudFront and ElastiCache. Scale\nin after the cache is fully populated.\n\nD. Configure an Auto Scaling group to scale out as traffic increases. Create a launch template to start new instances\n\nfrom a preconfigured Amazon Machine Image (AMI).\n\n", "image": "Screenshot 2024-12-27 at 12.05.30.png"}, {"text": "mesa\n\nQ894 A company collects and processes data from a vendor. The vendor stores its data in an Amazon RDS for MySQL\ndatabase in the vendor's own AWS account. The company\u2019s VPC does not have an internet gateway, an AWS Direct\nConnect connection, or an AWS Site-to-Site VPN connection. The company needs to access the data that is in the vendor\ndatabase. Which solution will meet this requirement?\n\nA. Instruct the vendor to sign up for the AWS Hosted Connection Direct Connect Program. Use VPC peering to connect\nthe company's VPC and the vendor's VPC.\n\nB. Configure a client VPN connection between the company's VPC and the vendor's VPC. Use VPC peering to connect\nthe company's VPC and the vendor's VPC.\n\nCc. Instruct the vendor to create a Network Load Balancer (NLB). Place the NLB in front of the Amazon RDS for MySQL\n\ndatabase. Use AWS PrivateLink to integrate the company's VPC and the vendor's VPC.\n\nD. Use AWS Transit Gateway to integrate the company's VPC and the vendor's VPC. Use VPC peering to connect the\ncompany\u2019s VPC and the vendor's VPC.\n", "image": "Screenshot 2024-12-27 at 12.06.49.png"}, {"text": "5514788\n\nQ1020 A company has deployed a multi-account strategy on AWS by using AWS Control Tower.The company has provided\nindividual AWS accounts to each of its developers.The company wants to implement controls to limit AWS resource costs\nthat the developers incur.Which solution will meet these requirements with the LEAST operational overhead?\n\nA. Instruct each developer to tag all their resources with a tag that has a key of CostCenter and a value of the\ndeveloper's name.Use the required-tags AWS Config managed rule to check for the tag.Create an AWS Lambda\nfunction to terminate resources that do not have the tag.Configure AWS Cost Explorer to send a daily report to each\ndeveloper to monitor their spending.\n\nB. Use AWS Budgets to establish budgets for each developer account.Set up budget alerts for actual and forecast\nvalues to notify developers when they exceed or expect to exceed their assigned budget.Use AWS Budgets actions\n\nto apply a DenyAll policy to the developer's IAM role to prevent additional resources from being launched when the\nassigned budget is reached.\n\nc. Use AWS Cost Explorer to monitor and report on costs for each developer account.Configure Cost Explorer to send a\ndaily report to each developer to monitor their spending.Use AWS Cost Anomaly Detection to detect anomalous\nspending and provide alerts.\n\nD. Use AWS Service Catalog to allow developers to launch resources within a limited cost range.Create AWS Lambda\nfunctions in each AWS account to stop running resources at the end of each work day.Configure the Lambda\nfunctions to resume the resources at the start of each work day.\n", "image": "Screenshot 2024-12-27 at 12.12.28.png"}, {"text": "B14\n\nQ239 A company is running a publicly accessible serverless application that uses Amazon API Gateway and AWS\nLambda.The application's traffic recently spiked due to fraudulent requests from botnets.Which steps should a solutions\narchitect take to block requests from unauthorized users? (Select TWO.)\n\nA. Create a usage plan with an API key that is shared with genuine users only.\n\nB. Integrate logic within the Lambda function to ignore the requests from fraudulent IP addresses.\n\nC.|mplement an AWS WAF rule to target malicious requests and trigger actions to filter them out.\n\nD. Convert the existing public API to a private API.Update the DNS records to redirect users to the new API endpoint.\n\nE. Create an IAM role for each user attempting to access the API.A user will assume the role when making the API call.\n", "image": "Screenshot 2024-12-27 at 11.40.15.png"}, {"text": "om\n\nQ72 A company needs to save the results from a medical trial to an Amazon S3 repository.The repository must allow a few\nscientists to add new files and must restrict all other users to read-only access.No users can have the ability to modify or\ndelete any files in the repository. The company must keep every file in the repository for a minimum of 1 year after its\ncreation date.Which solution will meet these requirements?\n\nA. Use S3 Object Lock In governance mode with a legal hold of 1 year\n\nB. Use S3 Object Lock in compliance mode with a retention period of 365 days.\n\nc. Use an IAM role to restrict all users from deleting or changing objects in the S3 bucket Use an S3 bucket policy to\nonly allow the IAM role\n\nD. Configure the S3 bucket to invoke an AWS Lambda function every tune an object is added Configure the function to\ntrack the hash of the saved object to that modified objects can be marked accordingly\n", "image": "Screenshot 2024-12-27 at 11.36.08.png"}, {"text": "5357\n\nQ752 An online video game company must maintain ultra-low latency for its game servers. The game servers run on\nAmazon EC2 instances. The company needs a solution that can handle millions of UDP internet traffic requests each\nsecond. Which solution will meet these requirements MOST cost-effectively?\n\nA. Configure an Application Load Balancer with the required protocol and ports for the internet traffic. Specify the EC2\ninstances as the targets.\n\nB. Configure a Gateway Load Balancer for the internet traffic. Specify the EC2 instances as the targets.\n\nc. Configure a Network Load Balancer with the required protocol and ports for the internet traffic. Specify the EC2\n\ninstances as the targets.\n\nD. Launch an identical set of game servers on EC2 instances in separate AWS Regions. Route internet traffic to both\nsets of EC2 instances.\n", "image": "Screenshot 2024-12-27 at 12.03.50.png"}, {"text": "eom\n\nQ71 A company sells ringtones created from clips of popular songs.The files containing the ringtones are stored in Amazon\nS3 Standard and are at least 128 KB in size.The company has millions of files, but downloads are infrequent for ringtones\nolder than 90 days.The company needs to save money on storage while keeping the most accessed files readily available\nfor its users.Which action should the company take to meet these requirements MOST cost-effectively?\n\nA. Configure S3 Standard-Infrequent Access (S3 Standard-lA) storage for the initial storage tier of the objects.\n\nB. Move the files to S3 Intelligent-Tiering and configure it to move objects to a less expensive storage tier after 90 days.\n\nCc. Configure S3 inventory to manage objects and move them to S3 Standard-Infrequent Access (S3 Standard-1A) after\n90 days.\n\nD. Implement an S3 Lifecycle policy that moves the objects from S3 Standard to S3 Standard-Infrequent Access (S3\nStandard-1A) after 90 days.\n", "image": "Screenshot 2024-12-27 at 11.35.59.png"}, {"text": "55278\n\nQ475 A payment processing company records all voice communication with its customers and stores the audio files in an\nAmazon S3 bucket.The company needs to capture the text from the audio files. The company must remove from the text\nany personally identifiable information (PII) that belongs to customers.What should a solutions architect do to meet these\n\nrequirements?\n\nA. Process the audio files by using Amazon Kinesis Video Streams.Use an AWS Lambda function to scan for known PIl\npatterns.\n\nB. When an audio file is uploaded to the S3 bucket, invoke an AWS Lambda function to start an Amazon Textract task to\nanalyze the call recordings.\n\nc. Configure an Amazon Transcribe transcription job with Pll redaction turned on.When an audio file is uploaded to the\n\nS3 bucket, invoke an AWS Lambda function to start the transcription job.Store the output in a separate S3 bucket.\n\nD. Create an Amazon Connect contact flow that ingests the audio files with transcription turned on.Embed an AWS\nLambda function to scan for known Pll patterns. Use Amazon EventBridge (Amazon CloudWatch Events) to start the\ncontact flow when an audio file is uploaded to the S3 bucket.\n", "image": "Screenshot 2024-12-27 at 11.42.12.png"}, {"text": "53139\n\nQ1034 A company runs a critical data analysis job each week before the first day of the work week.The job requires at least\n1 hour to complete the analysis.The job is stateful and cannot tolerate interruptions.The company needs a solution to run\n\nthe job on AWS.Which solution will meet these requirements?\n\nA. Create a container for the job.Schedule the job to run as an AWS Fargate task on an Amazon Elastic Container\n\nService (Amazon ECS) cluster by using Amazon EventBridge Scheduler.\n\nB. Configure the job to run in an AWS Lambda function.Create a scheduled rule in Amazon EventBridge to invoke the\n\nLambda function.\n\nCc. Configure an Auto Scaling group of Amazon EC2 Spot Instances that run Amazon Linux.Configure a crontab entry on\n\nthe instances to run the analysis.\n\nD. Configure an AWS DataSync task to run the job.Configure a cron expression to run the task on a schedule.\n", "image": "Screenshot 2024-12-27 at 12.11.46.png"}, {"text": "1458\n\nQ1022 A company has released a new version of its production application. The company's workload uses Amazon EC2,\nAWS Lambda, AWS Fargate, and Amazon SageMaker.The company wants to cost optimize the workload now that usage is\nat a steady state.The company wants to cover the most services with the fewest savings plans.Which combination of\nsavings plans will meet these requirements? (Choose two.)\n\nA. Purchase an EC2 Instance Savings Plan for Amazon EC2 and SageMaker.\n\nB. Purchase a Compute Savings Plan for Amazon EC2, Lambda, and SageMaker.\n\nCc. Purchase a SageMaker Savings Plan.\n\nD. Purchase a Compute Savings Plan for Lambda, Fargate, and Amazon EC2.\n\nE. Purchase an EC2 Instance Savings Plan for Amazon EC2 and Fargate.\n", "image": "Screenshot 2024-12-27 at 12.12.17.png"}, {"text": "35m\n\nQ539 A company uses AWS Organizations to run workloads within multiple AWS accounts.A tagging policy adds\ndepartment tags to AWS resources when the company creates tags.An accounting team needs to determine spending on\nAmazon EC2 consumption.The accounting team must determine which departments are responsible for the costs\nregardless ofAWS account.The accounting team has access to AWS Cost Explorer for all AWS accounts within the\norganization and needs to access all reports from Cost Explorer.Which solution meets these requirements in the MOST\n\noperationally efficient way?\n\nA. From the Organizations management account billing console, activate a user-defined cost allocation tag named\n\ndepartment.Create one cost report in Cost Explorer grouping by tag name, and filter by EC2.\n\nB. From the Organizations management account billing console, activate an AWS-defined cost allocation tag named\ndepartment.Create one cost report in Cost Explorer grouping by tag name, and filter by EC2.\n\nCc. From the Organizations member account billing console, activate a user-defined cost allocation tag named\ndepartment.Create one cost report in Cost Explorer grouping by the tag name, and filter by EC2.\n\nD. From the Organizations member account billing console, activate an AWS-defined cost allocation tag named\ndepartment.Create one cost report in Cost Explorer grouping by tag name, and filter by EC2.\n", "image": "Screenshot 2024-12-27 at 11.57.33.png"}, {"text": "51428\n\nQ1025 An ecommerce company wants to collect user clickstream data from the company's website for real-time\nanalysis.The website experiences uctuating traffic patterns throughout the day.The company needs a scalable solution\nthat can adapt to varying levels of traffic.Which solution will meet these requirements?\n\nA. Use a data stream in Amazon Kinesis Data Streams in on-demand mode to capture the clickstream data. Use AWS\n\nLambda to process the data in real time.\n\nB. Use Amazon Kinesis Data Firehose to capture the clickstream data.Use AWS Glue to process the data in real time.\nc. Use Amazon Kinesis Video Streams to capture the clickstream data.Use AWS Glue to process the data in real time.\n\nD. Use Amazon Managed Service for Apache Flink (previously known as Amazon Kinesis Data Analytics) to capture the\nclickstream data.Use AWS Lambda to process the data in real time.\n", "image": "Screenshot 2024-12-27 at 12.12.02.png"}, {"text": "8330\nQ526 A company stores data in PDF format in an Amazon S3 bucket.The company must follow a legal requirement to\nretain all new and existing data in Amazon S3 for 7 years.Which solution will meet these requirements with the LEAST\n\noperational overhead?\n\nA. Turn on the S3 Versioning feature for the S3 bucket.Configure S3 Lifecycle to delete the data after 7 years.Configure\nmulti-factor authentication (MFA) delete for all S3 objects.\n\nB. Turn on S3 Object Lock with governance retention mode for the S3 bucket.Set the retention period to expire after 7\nyears.Recopy all existing objects to bring the existing data into compliance.\n\nCc. Turn on S3 Object Lock with compliance retention mode for the S3 bucket.Set the retention period to expire after 7\nyears.Recopy all existing objects to bring the existing data into compliance.\n\nD. Turn on S3 Object Lock with compliance retention mode for the S3 bucket.Set the retention period to expire after 7\n\nyears.Use S3 Batch Operations to bring the existing data into compliance.\n\n", "image": "Screenshot 2024-12-27 at 11.49.32.png"}, {"text": "5597\n\nQ4 A company runs its production workload on an Amazon Aurora MySQL DB cluster that includes six Aurora Replicas.The\ncompany wants nearreal-time reporting queries from one of its departments to be automatically distributed across three of\nthe Aurora Replicas.Those three replicas have a different compute and memory specication from the rest of the DB\ncluster.Which solution meets these requirements?\n\nA. Create and use a custom endpoint for the workload\n\nB. Create a three-node cluster clone and use the reader endpoint\nCc. Use any of the instance endpoints for the selected three nodes\n\nD. Use the reader endpoint to automatically distribute the read-only workload\n", "image": "Screenshot 2024-12-27 at 12.07.37.png"}, {"text": "om\n\nQ145 A company wants to improve its ability to clone large amounts of production data into a test environment in the same\nAWS Region.The data is stored in Amazon EC2 instances on Amazon Elastic Block Store (Amazon EBS)\nvolumes.Modifications to the cloned data must not affect the production environment.The software that accesses this data\nrequires consistently high I/O performance.A solutions architect needs to minimize the time that is required to clone the\nproduction data into the test environment.Which solution will meet these requirements?\n\nA. Take EBS snapshots of the production EBS volumes.Restore the snapshots onto EC2 instance store volumes in the\ntest environment.\n\nB. Configure the production EBS volumes to use the EBS Multi-Attach feature.Take EBS snapshots of the production\nEBS volumes.Attach the production EBS volumes to the EC2 instances in the test environment.\n\nCc. Take EBS snapshots of the production EBS volumes.Create and initialize new EBS volumes.Attach the new EBS\nvolumes to EC2 instances in the test environment before restoring the volumes from the production EBS snapshots.\n\nD. Take EBS snapshots of the production EBS volumes.Turn on the EBS fast snapshot restore feature on the EBS\n\nsnapshots.Restore the snapshots into new EBS volumes.Attach the new EBS volumes to EC2 instances in the test\nenvironment.\n\n", "image": "Screenshot 2024-12-27 at 11.36.37.png"}]